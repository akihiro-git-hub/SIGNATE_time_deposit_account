{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso,Ridge\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.layers.advanced_activations import ReLU, PReLU\n",
    "from keras.optimizers import SGD, Adam\n",
    "from scipy.stats import mode\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss,roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers.advanced_activations import ReLU, PReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.read_csv('C:\\\\Users\\\\odoru\\\\SIGNATE_time_deposit_account\\\\dataset\\\\train_x_xgboost.csv')\n",
    "train_y = pd.read_csv('C:\\\\Users\\\\odoru\\\\SIGNATE_time_deposit_account\\\\dataset\\\\train_y_xgboost.csv')\n",
    "test_x = pd.read_csv('C:\\\\Users\\\\odoru\\\\SIGNATE_time_deposit_account\\\\dataset\\\\test_x_xgboost.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27095</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27096</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27097</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27098</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27099</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1  2  3\n",
       "0      0  1  0  0\n",
       "1      0  0  0  1\n",
       "2      1  0  0  0\n",
       "3      1  0  0  0\n",
       "4      0  0  0  1\n",
       "...   .. .. .. ..\n",
       "27095  0  0  0  1\n",
       "27096  0  0  0  1\n",
       "27097  0  0  0  1\n",
       "27098  0  0  0  1\n",
       "27099  0  0  0  1\n",
       "\n",
       "[27100 rows x 4 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_poutcome_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_poutcome_xg = train_x.drop(['poutcome'], axis=1)\n",
    "train_y_poutcome_xg = train_x['poutcome']\n",
    "train_y_poutcome_dummies = pd.get_dummies(train_x['poutcome'])\n",
    "train_y_poutcome_f = train_y_poutcome_dummies[0]\n",
    "train_y_poutcome_o = train_y_poutcome_dummies[1]\n",
    "train_y_poutcome_s = train_y_poutcome_dummies[2]\n",
    "train_y_poutcome_u = train_y_poutcome_dummies[3]\n",
    "test_x_poutcome_xg = test_x.drop(['poutcome'], axis=1)\n",
    "test_y_poutcome_xg = test_x['poutcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_nn = pd.read_csv('.//dataset//train_x_NN.csv')\n",
    "train_y_nn = pd.read_csv('.//dataset//train_y_NN.csv')\n",
    "test_x_nn = pd.read_csv('.//dataset//test_x_NN.csv')\n",
    "train_x_nn_poutcome = train_x_nn.iloc[:,:48]\n",
    "train_y_nn_poutcome = train_x_nn.iloc[:,48:]\n",
    "test_x_nn_poutcome = test_x_nn.iloc[:,:48]\n",
    "test_y_nn_poutcome = test_x_nn.iloc[:,48:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poutcome_failure</th>\n",
       "      <th>poutcome_other</th>\n",
       "      <th>poutcome_success</th>\n",
       "      <th>poutcome_unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27095</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27096</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27097</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27098</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27099</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       poutcome_failure  poutcome_other  poutcome_success  poutcome_unknown\n",
       "0                     0               1                 0                 0\n",
       "1                     0               0                 0                 1\n",
       "2                     1               0                 0                 0\n",
       "3                     1               0                 0                 0\n",
       "4                     0               0                 0                 1\n",
       "...                 ...             ...               ...               ...\n",
       "27095                 0               0                 0                 1\n",
       "27096                 0               0                 0                 1\n",
       "27097                 0               0                 0                 1\n",
       "27098                 0               0                 0                 1\n",
       "27099                 0               0                 0                 1\n",
       "\n",
       "[27100 rows x 4 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_nn_poutcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cv_classfier(model,train_x, train_y, test_x):\n",
    "    \n",
    "\n",
    "    preds = []\n",
    "    preds_test = []\n",
    "    va_idxes = []\n",
    "\n",
    "\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state= 71)\n",
    "    for i , (tr_idx, va_idx) in enumerate (kf.split(train_x)):\n",
    "        tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "        tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "        model.fit(tr_x, tr_y, va_x, va_y)\n",
    "        pred = model.predict(va_x)\n",
    "        preds.append(pred)\n",
    "        pred_test = model.predict(test_x)\n",
    "        preds_test.append(pred_test)\n",
    "        va_idxes.append(va_idx)\n",
    "\n",
    "    va_idxes = np.concatenate(va_idxes)\n",
    "    preds = np.concatenate(preds)\n",
    "    order = np.argsort(va_idxes)\n",
    "    pred_train = preds[order]\n",
    "\n",
    "    tmp = np.stack(preds_test, axis =1)\n",
    "    mode_test, mode_counts = mode(tmp, axis=1)\n",
    "\n",
    "    preds_test = mode_test\n",
    "    preds_size = preds_test.shape[0]\n",
    "    preds_test = preds_test.reshape(preds_size,)\n",
    "    \n",
    "    return pred_train, preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cv(model, train_x, train_y, test_x):\n",
    "    preds = []\n",
    "    preds_test = []\n",
    "    va_idxes = []\n",
    "    \n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=71)\n",
    "    for i , (tr_idx, va_idx) in enumerate(kf.split(train_x)):\n",
    "        tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "        tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "        model.fit(tr_x, tr_y, va_x, va_y)\n",
    "        pred = model.predict(va_x)\n",
    "        preds.append(pred)\n",
    "        pred_test = model.predict(test_x)\n",
    "        preds_test.append(pred_test)\n",
    "        va_idxes.append(va_idx)\n",
    "        \n",
    "    va_idxes = np.concatenate(va_idxes)\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    order = np.argsort(va_idxes)\n",
    "    pred_train = preds[order]\n",
    "    \n",
    "    preds_test = np.mean(preds_test, axis=0)\n",
    "    \n",
    "    return pred_train, preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Model1NNsoftmax, Model1NN2softmax, Model1xgbsoftmax, Model1xgb2softmax,Model2KNN_p, Model1ramdom, Model3logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.4237 - accuracy: 0.8696 - val_loss: 0.3397 - val_accuracy: 0.8852\n",
      "Epoch 2/100\n",
      "159/159 [==============================] - 1s 5ms/step - loss: 0.3385 - accuracy: 0.8869 - val_loss: 0.3300 - val_accuracy: 0.8893\n",
      "Epoch 3/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.3193 - accuracy: 0.8909 - val_loss: 0.3251 - val_accuracy: 0.8928\n",
      "Epoch 4/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.3124 - accuracy: 0.8922 - val_loss: 0.3260 - val_accuracy: 0.8894\n",
      "Epoch 5/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.3070 - accuracy: 0.8942 - val_loss: 0.3230 - val_accuracy: 0.8939\n",
      "Epoch 6/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2975 - accuracy: 0.8948 - val_loss: 0.3240 - val_accuracy: 0.8903\n",
      "Epoch 7/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2907 - accuracy: 0.8974 - val_loss: 0.3233 - val_accuracy: 0.8912\n",
      "Epoch 8/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2867 - accuracy: 0.8968 - val_loss: 0.3255 - val_accuracy: 0.8892\n",
      "Epoch 9/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2820 - accuracy: 0.9003 - val_loss: 0.3233 - val_accuracy: 0.8892\n",
      "Epoch 10/100\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 0.2781 - accuracy: 0.9013 - val_loss: 0.3236 - val_accuracy: 0.8902\n",
      "Epoch 11/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2777 - accuracy: 0.9005 - val_loss: 0.3248 - val_accuracy: 0.8909\n",
      "Epoch 12/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2705 - accuracy: 0.9007 - val_loss: 0.3221 - val_accuracy: 0.8920\n",
      "Epoch 13/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2680 - accuracy: 0.9009 - val_loss: 0.3273 - val_accuracy: 0.8911\n",
      "Epoch 14/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2588 - accuracy: 0.9057 - val_loss: 0.3309 - val_accuracy: 0.8858\n",
      "Epoch 15/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2562 - accuracy: 0.9060 - val_loss: 0.3287 - val_accuracy: 0.8900\n",
      "Epoch 16/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2540 - accuracy: 0.9060 - val_loss: 0.3328 - val_accuracy: 0.8883\n",
      "Epoch 17/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2520 - accuracy: 0.9068 - val_loss: 0.3374 - val_accuracy: 0.8894\n",
      "Epoch 18/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2452 - accuracy: 0.9078 - val_loss: 0.3348 - val_accuracy: 0.8874\n",
      "Epoch 19/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2417 - accuracy: 0.9104 - val_loss: 0.3384 - val_accuracy: 0.8831\n",
      "Epoch 20/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2369 - accuracy: 0.9105 - val_loss: 0.3396 - val_accuracy: 0.8852\n",
      "Epoch 21/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2323 - accuracy: 0.9129 - val_loss: 0.3389 - val_accuracy: 0.8858\n",
      "Epoch 22/100\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 0.2304 - accuracy: 0.9128 - val_loss: 0.3412 - val_accuracy: 0.8890\n",
      "Epoch 23/100\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 0.2257 - accuracy: 0.9157 - val_loss: 0.3416 - val_accuracy: 0.8866\n",
      "Epoch 24/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2214 - accuracy: 0.9165 - val_loss: 0.3524 - val_accuracy: 0.8855\n",
      "Epoch 25/100\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 0.2155 - accuracy: 0.9177 - val_loss: 0.3509 - val_accuracy: 0.8809\n",
      "Epoch 26/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2120 - accuracy: 0.9202 - val_loss: 0.3529 - val_accuracy: 0.8843\n",
      "Epoch 27/100\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 0.2098 - accuracy: 0.9200 - val_loss: 0.3558 - val_accuracy: 0.8868\n",
      "Epoch 28/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2054 - accuracy: 0.9200 - val_loss: 0.3554 - val_accuracy: 0.8840\n",
      "Epoch 29/100\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 0.1992 - accuracy: 0.9228 - val_loss: 0.3615 - val_accuracy: 0.8878\n",
      "Epoch 30/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.1964 - accuracy: 0.9255 - val_loss: 0.3564 - val_accuracy: 0.8827\n",
      "Epoch 31/100\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 0.1968 - accuracy: 0.9237 - val_loss: 0.3647 - val_accuracy: 0.8838\n",
      "Epoch 32/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.1934 - accuracy: 0.9261 - val_loss: 0.3691 - val_accuracy: 0.8846\n",
      "Epoch 1/100\n",
      "159/159 [==============================] - 1s 5ms/step - loss: 0.4157 - accuracy: 0.8711 - val_loss: 0.3310 - val_accuracy: 0.8892\n",
      "Epoch 2/100\n",
      "159/159 [==============================] - 1s 5ms/step - loss: 0.3399 - accuracy: 0.8853 - val_loss: 0.3190 - val_accuracy: 0.8928\n",
      "Epoch 3/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.3266 - accuracy: 0.8899 - val_loss: 0.3189 - val_accuracy: 0.8930\n",
      "Epoch 4/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.3210 - accuracy: 0.8905 - val_loss: 0.3153 - val_accuracy: 0.8955\n",
      "Epoch 5/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.3132 - accuracy: 0.8919 - val_loss: 0.3129 - val_accuracy: 0.8940\n",
      "Epoch 6/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.3057 - accuracy: 0.8915 - val_loss: 0.3066 - val_accuracy: 0.8958\n",
      "Epoch 7/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2994 - accuracy: 0.8953 - val_loss: 0.3070 - val_accuracy: 0.8959\n",
      "Epoch 8/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2930 - accuracy: 0.8961 - val_loss: 0.3099 - val_accuracy: 0.8952\n",
      "Epoch 9/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2898 - accuracy: 0.8961 - val_loss: 0.3085 - val_accuracy: 0.8955\n",
      "Epoch 10/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2839 - accuracy: 0.8979 - val_loss: 0.3143 - val_accuracy: 0.8918\n",
      "Epoch 11/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2825 - accuracy: 0.8989 - val_loss: 0.3093 - val_accuracy: 0.8956\n",
      "Epoch 12/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2756 - accuracy: 0.9006 - val_loss: 0.3093 - val_accuracy: 0.8948\n",
      "Epoch 13/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2729 - accuracy: 0.9010 - val_loss: 0.3114 - val_accuracy: 0.8918\n",
      "Epoch 14/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2693 - accuracy: 0.8999 - val_loss: 0.3150 - val_accuracy: 0.8917\n",
      "Epoch 15/100\n",
      "159/159 [==============================] - 1s 5ms/step - loss: 0.2625 - accuracy: 0.9033 - val_loss: 0.3157 - val_accuracy: 0.8940\n",
      "Epoch 16/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2584 - accuracy: 0.9049 - val_loss: 0.3190 - val_accuracy: 0.8906\n",
      "Epoch 17/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2534 - accuracy: 0.9076 - val_loss: 0.3167 - val_accuracy: 0.8928\n",
      "Epoch 18/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2489 - accuracy: 0.9075 - val_loss: 0.3191 - val_accuracy: 0.8923\n",
      "Epoch 19/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2452 - accuracy: 0.9093 - val_loss: 0.3177 - val_accuracy: 0.8903\n",
      "Epoch 20/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2404 - accuracy: 0.9119 - val_loss: 0.3250 - val_accuracy: 0.8937\n",
      "Epoch 21/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2385 - accuracy: 0.9097 - val_loss: 0.3238 - val_accuracy: 0.8928\n",
      "Epoch 22/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2346 - accuracy: 0.9120 - val_loss: 0.3273 - val_accuracy: 0.8920\n",
      "Epoch 23/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2311 - accuracy: 0.9130 - val_loss: 0.3322 - val_accuracy: 0.8925\n",
      "Epoch 24/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2227 - accuracy: 0.9160 - val_loss: 0.3354 - val_accuracy: 0.8918\n",
      "Epoch 25/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2220 - accuracy: 0.9163 - val_loss: 0.3380 - val_accuracy: 0.8903\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2190 - accuracy: 0.9184 - val_loss: 0.3368 - val_accuracy: 0.8893\n",
      "Epoch 1/100\n",
      "159/159 [==============================] - 1s 5ms/step - loss: 0.4172 - accuracy: 0.8705 - val_loss: 0.3395 - val_accuracy: 0.8874\n",
      "Epoch 2/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.3364 - accuracy: 0.8878 - val_loss: 0.3335 - val_accuracy: 0.8872\n",
      "Epoch 3/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.3237 - accuracy: 0.8895 - val_loss: 0.3262 - val_accuracy: 0.8905\n",
      "Epoch 4/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.3134 - accuracy: 0.8931 - val_loss: 0.3214 - val_accuracy: 0.8914\n",
      "Epoch 5/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.3069 - accuracy: 0.8927 - val_loss: 0.3244 - val_accuracy: 0.8877\n",
      "Epoch 6/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.3014 - accuracy: 0.8950 - val_loss: 0.3240 - val_accuracy: 0.8899\n",
      "Epoch 7/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2956 - accuracy: 0.8949 - val_loss: 0.3215 - val_accuracy: 0.8921\n",
      "Epoch 8/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2910 - accuracy: 0.8959 - val_loss: 0.3251 - val_accuracy: 0.8865\n",
      "Epoch 9/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2856 - accuracy: 0.8980 - val_loss: 0.3203 - val_accuracy: 0.8903\n",
      "Epoch 10/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2829 - accuracy: 0.8993 - val_loss: 0.3176 - val_accuracy: 0.8909\n",
      "Epoch 11/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2792 - accuracy: 0.8997 - val_loss: 0.3251 - val_accuracy: 0.8939\n",
      "Epoch 12/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2760 - accuracy: 0.9001 - val_loss: 0.3229 - val_accuracy: 0.8906\n",
      "Epoch 13/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2684 - accuracy: 0.9029 - val_loss: 0.3198 - val_accuracy: 0.8902\n",
      "Epoch 14/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2664 - accuracy: 0.9024 - val_loss: 0.3259 - val_accuracy: 0.8936\n",
      "Epoch 15/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2627 - accuracy: 0.9038 - val_loss: 0.3241 - val_accuracy: 0.8905\n",
      "Epoch 16/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2578 - accuracy: 0.9064 - val_loss: 0.3220 - val_accuracy: 0.8917\n",
      "Epoch 17/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2571 - accuracy: 0.9058 - val_loss: 0.3232 - val_accuracy: 0.8894\n",
      "Epoch 18/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2511 - accuracy: 0.9086 - val_loss: 0.3306 - val_accuracy: 0.8924\n",
      "Epoch 19/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2463 - accuracy: 0.9079 - val_loss: 0.3280 - val_accuracy: 0.8884\n",
      "Epoch 20/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2418 - accuracy: 0.9097 - val_loss: 0.3284 - val_accuracy: 0.8893\n",
      "Epoch 21/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2373 - accuracy: 0.9104 - val_loss: 0.3405 - val_accuracy: 0.8914\n",
      "Epoch 22/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2339 - accuracy: 0.9134 - val_loss: 0.3327 - val_accuracy: 0.8900\n",
      "Epoch 23/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2312 - accuracy: 0.9133 - val_loss: 0.3315 - val_accuracy: 0.8909\n",
      "Epoch 24/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2247 - accuracy: 0.9159 - val_loss: 0.3410 - val_accuracy: 0.8886\n",
      "Epoch 25/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2219 - accuracy: 0.9153 - val_loss: 0.3374 - val_accuracy: 0.8871\n",
      "Epoch 26/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2170 - accuracy: 0.9169 - val_loss: 0.3389 - val_accuracy: 0.8906\n",
      "Epoch 27/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2109 - accuracy: 0.9203 - val_loss: 0.3426 - val_accuracy: 0.8892\n",
      "Epoch 28/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2080 - accuracy: 0.9204 - val_loss: 0.3431 - val_accuracy: 0.8853\n",
      "Epoch 29/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2064 - accuracy: 0.9199 - val_loss: 0.3450 - val_accuracy: 0.8889\n",
      "Epoch 30/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2037 - accuracy: 0.9228 - val_loss: 0.3435 - val_accuracy: 0.8883\n",
      "Epoch 1/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.4111 - accuracy: 0.8729 - val_loss: 0.3461 - val_accuracy: 0.8840\n",
      "Epoch 2/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.3348 - accuracy: 0.8880 - val_loss: 0.3360 - val_accuracy: 0.8846\n",
      "Epoch 3/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.3227 - accuracy: 0.8902 - val_loss: 0.3303 - val_accuracy: 0.8853\n",
      "Epoch 4/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.3108 - accuracy: 0.8928 - val_loss: 0.3310 - val_accuracy: 0.8866\n",
      "Epoch 5/100\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 0.3047 - accuracy: 0.8949 - val_loss: 0.3310 - val_accuracy: 0.8883\n",
      "Epoch 6/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2961 - accuracy: 0.8956 - val_loss: 0.3248 - val_accuracy: 0.8896\n",
      "Epoch 7/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2957 - accuracy: 0.8979 - val_loss: 0.3264 - val_accuracy: 0.8861\n",
      "Epoch 8/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2883 - accuracy: 0.8977 - val_loss: 0.3292 - val_accuracy: 0.8880\n",
      "Epoch 9/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2824 - accuracy: 0.8989 - val_loss: 0.3332 - val_accuracy: 0.8887\n",
      "Epoch 10/100\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 0.2793 - accuracy: 0.8996 - val_loss: 0.3301 - val_accuracy: 0.8852\n",
      "Epoch 11/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2760 - accuracy: 0.9018 - val_loss: 0.3332 - val_accuracy: 0.8861\n",
      "Epoch 12/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2701 - accuracy: 0.9038 - val_loss: 0.3305 - val_accuracy: 0.8847\n",
      "Epoch 13/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2661 - accuracy: 0.9040 - val_loss: 0.3317 - val_accuracy: 0.8863\n",
      "Epoch 14/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2609 - accuracy: 0.9048 - val_loss: 0.3360 - val_accuracy: 0.8827\n",
      "Epoch 15/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2573 - accuracy: 0.9080 - val_loss: 0.3392 - val_accuracy: 0.8840\n",
      "Epoch 16/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2552 - accuracy: 0.9057 - val_loss: 0.3447 - val_accuracy: 0.8837\n",
      "Epoch 17/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2480 - accuracy: 0.9102 - val_loss: 0.3452 - val_accuracy: 0.8847\n",
      "Epoch 18/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2441 - accuracy: 0.9091 - val_loss: 0.3453 - val_accuracy: 0.8822\n",
      "Epoch 19/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2440 - accuracy: 0.9126 - val_loss: 0.3490 - val_accuracy: 0.8846\n",
      "Epoch 20/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2354 - accuracy: 0.9138 - val_loss: 0.3525 - val_accuracy: 0.8818\n",
      "Epoch 21/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2343 - accuracy: 0.9138 - val_loss: 0.3513 - val_accuracy: 0.8830\n",
      "Epoch 22/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2296 - accuracy: 0.9142 - val_loss: 0.3560 - val_accuracy: 0.8832\n",
      "Epoch 23/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2271 - accuracy: 0.9143 - val_loss: 0.3620 - val_accuracy: 0.8799\n",
      "Epoch 24/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2201 - accuracy: 0.9201 - val_loss: 0.3573 - val_accuracy: 0.8827\n",
      "Epoch 25/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2175 - accuracy: 0.9169 - val_loss: 0.3632 - val_accuracy: 0.8813\n",
      "Epoch 26/100\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 0.2154 - accuracy: 0.9171 - val_loss: 0.3570 - val_accuracy: 0.8813\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212/212 [==============================] - 1s 4ms/step - loss: 0.5021 - accuracy: 0.8502 - val_loss: 0.3562 - val_accuracy: 0.8825\n",
      "Epoch 2/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3894 - accuracy: 0.8760 - val_loss: 0.3466 - val_accuracy: 0.8824\n",
      "Epoch 3/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3762 - accuracy: 0.8801 - val_loss: 0.3375 - val_accuracy: 0.8850\n",
      "Epoch 4/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3638 - accuracy: 0.8817 - val_loss: 0.3383 - val_accuracy: 0.8869\n",
      "Epoch 5/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3550 - accuracy: 0.8832 - val_loss: 0.3325 - val_accuracy: 0.8884\n",
      "Epoch 6/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3546 - accuracy: 0.8841 - val_loss: 0.3301 - val_accuracy: 0.8906\n",
      "Epoch 7/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3459 - accuracy: 0.8841 - val_loss: 0.3268 - val_accuracy: 0.8914\n",
      "Epoch 8/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3445 - accuracy: 0.8843 - val_loss: 0.3269 - val_accuracy: 0.8899\n",
      "Epoch 9/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3380 - accuracy: 0.8871 - val_loss: 0.3235 - val_accuracy: 0.8917\n",
      "Epoch 10/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3372 - accuracy: 0.8860 - val_loss: 0.3217 - val_accuracy: 0.8933\n",
      "Epoch 11/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3333 - accuracy: 0.8883 - val_loss: 0.3217 - val_accuracy: 0.8918\n",
      "Epoch 12/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3282 - accuracy: 0.8883 - val_loss: 0.3205 - val_accuracy: 0.8917\n",
      "Epoch 13/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3290 - accuracy: 0.8882 - val_loss: 0.3246 - val_accuracy: 0.8956\n",
      "Epoch 14/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3276 - accuracy: 0.8887 - val_loss: 0.3193 - val_accuracy: 0.8908\n",
      "Epoch 15/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3228 - accuracy: 0.8885 - val_loss: 0.3186 - val_accuracy: 0.8927\n",
      "Epoch 16/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3236 - accuracy: 0.8895 - val_loss: 0.3171 - val_accuracy: 0.8918\n",
      "Epoch 17/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3234 - accuracy: 0.8895 - val_loss: 0.3172 - val_accuracy: 0.8936\n",
      "Epoch 18/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3219 - accuracy: 0.8889 - val_loss: 0.3184 - val_accuracy: 0.8939\n",
      "Epoch 19/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3197 - accuracy: 0.8911 - val_loss: 0.3138 - val_accuracy: 0.8937\n",
      "Epoch 20/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3149 - accuracy: 0.8910 - val_loss: 0.3140 - val_accuracy: 0.8937\n",
      "Epoch 21/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3194 - accuracy: 0.8910 - val_loss: 0.3144 - val_accuracy: 0.8924\n",
      "Epoch 22/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3158 - accuracy: 0.8915 - val_loss: 0.3116 - val_accuracy: 0.8948\n",
      "Epoch 23/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3136 - accuracy: 0.8914 - val_loss: 0.3138 - val_accuracy: 0.8946\n",
      "Epoch 24/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3111 - accuracy: 0.8920 - val_loss: 0.3130 - val_accuracy: 0.8943\n",
      "Epoch 25/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3141 - accuracy: 0.8898 - val_loss: 0.3087 - val_accuracy: 0.8952\n",
      "Epoch 26/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3128 - accuracy: 0.8901 - val_loss: 0.3117 - val_accuracy: 0.8936\n",
      "Epoch 27/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3059 - accuracy: 0.8945 - val_loss: 0.3120 - val_accuracy: 0.8948\n",
      "Epoch 28/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3083 - accuracy: 0.8920 - val_loss: 0.3120 - val_accuracy: 0.8940\n",
      "Epoch 29/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3076 - accuracy: 0.8914 - val_loss: 0.3081 - val_accuracy: 0.8948\n",
      "Epoch 30/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3050 - accuracy: 0.8927 - val_loss: 0.3094 - val_accuracy: 0.8945\n",
      "Epoch 31/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3091 - accuracy: 0.8925 - val_loss: 0.3109 - val_accuracy: 0.8945\n",
      "Epoch 32/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3029 - accuracy: 0.8925 - val_loss: 0.3104 - val_accuracy: 0.8937\n",
      "Epoch 33/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3020 - accuracy: 0.8941 - val_loss: 0.3069 - val_accuracy: 0.8942\n",
      "Epoch 34/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3043 - accuracy: 0.8932 - val_loss: 0.3073 - val_accuracy: 0.8942\n",
      "Epoch 35/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3054 - accuracy: 0.8923 - val_loss: 0.3070 - val_accuracy: 0.8940\n",
      "Epoch 36/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3002 - accuracy: 0.8936 - val_loss: 0.3054 - val_accuracy: 0.8934\n",
      "Epoch 37/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3009 - accuracy: 0.8927 - val_loss: 0.3064 - val_accuracy: 0.8930\n",
      "Epoch 38/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3012 - accuracy: 0.8927 - val_loss: 0.3032 - val_accuracy: 0.8956\n",
      "Epoch 39/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2948 - accuracy: 0.8959 - val_loss: 0.3057 - val_accuracy: 0.8942\n",
      "Epoch 40/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2936 - accuracy: 0.8955 - val_loss: 0.3033 - val_accuracy: 0.8958\n",
      "Epoch 41/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2951 - accuracy: 0.8929 - val_loss: 0.3039 - val_accuracy: 0.8943\n",
      "Epoch 42/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2951 - accuracy: 0.8943 - val_loss: 0.3020 - val_accuracy: 0.8940\n",
      "Epoch 43/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2952 - accuracy: 0.8944 - val_loss: 0.3029 - val_accuracy: 0.8948\n",
      "Epoch 44/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2909 - accuracy: 0.8959 - val_loss: 0.3029 - val_accuracy: 0.8942\n",
      "Epoch 45/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2927 - accuracy: 0.8948 - val_loss: 0.3005 - val_accuracy: 0.8959\n",
      "Epoch 46/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2901 - accuracy: 0.8975 - val_loss: 0.3013 - val_accuracy: 0.8961\n",
      "Epoch 47/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2904 - accuracy: 0.8946 - val_loss: 0.3008 - val_accuracy: 0.8958\n",
      "Epoch 48/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2902 - accuracy: 0.8971 - val_loss: 0.2989 - val_accuracy: 0.8951\n",
      "Epoch 49/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2896 - accuracy: 0.8949 - val_loss: 0.3010 - val_accuracy: 0.8949\n",
      "Epoch 50/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2832 - accuracy: 0.8966 - val_loss: 0.2988 - val_accuracy: 0.8979\n",
      "Epoch 51/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2855 - accuracy: 0.8968 - val_loss: 0.2995 - val_accuracy: 0.8965\n",
      "Epoch 52/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2855 - accuracy: 0.8972 - val_loss: 0.2969 - val_accuracy: 0.8983\n",
      "Epoch 53/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2856 - accuracy: 0.8968 - val_loss: 0.2973 - val_accuracy: 0.8971\n",
      "Epoch 54/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2866 - accuracy: 0.8976 - val_loss: 0.2949 - val_accuracy: 0.8974\n",
      "Epoch 55/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2789 - accuracy: 0.8971 - val_loss: 0.2955 - val_accuracy: 0.8959\n",
      "Epoch 56/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2784 - accuracy: 0.8993 - val_loss: 0.2954 - val_accuracy: 0.8976\n",
      "Epoch 57/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2792 - accuracy: 0.8985 - val_loss: 0.2928 - val_accuracy: 0.8964\n",
      "Epoch 58/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2783 - accuracy: 0.8996 - val_loss: 0.2937 - val_accuracy: 0.8976\n",
      "Epoch 59/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2761 - accuracy: 0.9008 - val_loss: 0.2930 - val_accuracy: 0.8974\n",
      "Epoch 60/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2735 - accuracy: 0.8992 - val_loss: 0.2924 - val_accuracy: 0.8976\n",
      "Epoch 61/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2746 - accuracy: 0.8999 - val_loss: 0.2892 - val_accuracy: 0.8979\n",
      "Epoch 62/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2764 - accuracy: 0.8990 - val_loss: 0.2901 - val_accuracy: 0.8990\n",
      "Epoch 63/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2734 - accuracy: 0.9006 - val_loss: 0.2911 - val_accuracy: 0.8964\n",
      "Epoch 64/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2722 - accuracy: 0.9027 - val_loss: 0.2902 - val_accuracy: 0.8987\n",
      "Epoch 65/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2764 - accuracy: 0.9007 - val_loss: 0.2927 - val_accuracy: 0.8985\n",
      "Epoch 66/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2713 - accuracy: 0.9008 - val_loss: 0.2910 - val_accuracy: 0.8985\n",
      "Epoch 67/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2721 - accuracy: 0.9032 - val_loss: 0.2916 - val_accuracy: 0.8989\n",
      "Epoch 68/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2681 - accuracy: 0.9028 - val_loss: 0.2926 - val_accuracy: 0.8989\n",
      "Epoch 69/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2704 - accuracy: 0.9021 - val_loss: 0.2884 - val_accuracy: 0.8985\n",
      "Epoch 70/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2710 - accuracy: 0.9015 - val_loss: 0.2883 - val_accuracy: 0.9001\n",
      "Epoch 71/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2696 - accuracy: 0.9027 - val_loss: 0.2872 - val_accuracy: 0.9004\n",
      "Epoch 72/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2675 - accuracy: 0.9015 - val_loss: 0.2872 - val_accuracy: 0.9010\n",
      "Epoch 73/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2692 - accuracy: 0.9046 - val_loss: 0.2872 - val_accuracy: 0.9001\n",
      "Epoch 74/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2644 - accuracy: 0.9036 - val_loss: 0.2883 - val_accuracy: 0.9013\n",
      "Epoch 75/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2657 - accuracy: 0.9024 - val_loss: 0.2890 - val_accuracy: 0.9011\n",
      "Epoch 76/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2651 - accuracy: 0.9029 - val_loss: 0.2897 - val_accuracy: 0.9001\n",
      "Epoch 77/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2658 - accuracy: 0.9033 - val_loss: 0.2944 - val_accuracy: 0.8995\n",
      "Epoch 78/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2666 - accuracy: 0.9027 - val_loss: 0.2930 - val_accuracy: 0.8989\n",
      "Epoch 79/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2643 - accuracy: 0.9033 - val_loss: 0.2906 - val_accuracy: 0.8995\n",
      "Epoch 80/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2626 - accuracy: 0.9033 - val_loss: 0.2904 - val_accuracy: 0.8998\n",
      "Epoch 81/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2643 - accuracy: 0.9022 - val_loss: 0.2908 - val_accuracy: 0.9001\n",
      "Epoch 82/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2675 - accuracy: 0.9048 - val_loss: 0.2899 - val_accuracy: 0.8990\n",
      "Epoch 83/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2630 - accuracy: 0.9039 - val_loss: 0.2903 - val_accuracy: 0.8999\n",
      "Epoch 84/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2657 - accuracy: 0.9030 - val_loss: 0.2908 - val_accuracy: 0.8982\n",
      "Epoch 85/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2602 - accuracy: 0.9048 - val_loss: 0.2886 - val_accuracy: 0.8993\n",
      "Epoch 86/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2598 - accuracy: 0.9051 - val_loss: 0.2905 - val_accuracy: 0.8985\n",
      "Epoch 87/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2599 - accuracy: 0.9048 - val_loss: 0.2906 - val_accuracy: 0.8990\n",
      "Epoch 88/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2617 - accuracy: 0.9042 - val_loss: 0.2899 - val_accuracy: 0.9007\n",
      "Epoch 89/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2602 - accuracy: 0.9061 - val_loss: 0.2872 - val_accuracy: 0.9026\n",
      "Epoch 90/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2622 - accuracy: 0.9043 - val_loss: 0.2885 - val_accuracy: 0.9008\n",
      "Epoch 91/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2587 - accuracy: 0.9050 - val_loss: 0.2899 - val_accuracy: 0.8998\n",
      "Epoch 1/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.4887 - accuracy: 0.8576 - val_loss: 0.3619 - val_accuracy: 0.8819\n",
      "Epoch 2/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3967 - accuracy: 0.8754 - val_loss: 0.3463 - val_accuracy: 0.8877\n",
      "Epoch 3/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3778 - accuracy: 0.8773 - val_loss: 0.3450 - val_accuracy: 0.8914\n",
      "Epoch 4/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3654 - accuracy: 0.8800 - val_loss: 0.3266 - val_accuracy: 0.8908\n",
      "Epoch 5/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3617 - accuracy: 0.8807 - val_loss: 0.3205 - val_accuracy: 0.8909\n",
      "Epoch 6/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3545 - accuracy: 0.8816 - val_loss: 0.3199 - val_accuracy: 0.8934\n",
      "Epoch 7/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3494 - accuracy: 0.8835 - val_loss: 0.3175 - val_accuracy: 0.8952\n",
      "Epoch 8/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3460 - accuracy: 0.8847 - val_loss: 0.3162 - val_accuracy: 0.8958\n",
      "Epoch 9/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3425 - accuracy: 0.8853 - val_loss: 0.3129 - val_accuracy: 0.8958\n",
      "Epoch 10/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3419 - accuracy: 0.8844 - val_loss: 0.3225 - val_accuracy: 0.8964\n",
      "Epoch 11/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3368 - accuracy: 0.8857 - val_loss: 0.3086 - val_accuracy: 0.8964\n",
      "Epoch 12/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3345 - accuracy: 0.8861 - val_loss: 0.3095 - val_accuracy: 0.8987\n",
      "Epoch 13/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3328 - accuracy: 0.8870 - val_loss: 0.3082 - val_accuracy: 0.8968\n",
      "Epoch 14/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3277 - accuracy: 0.8890 - val_loss: 0.3035 - val_accuracy: 0.8995\n",
      "Epoch 15/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3309 - accuracy: 0.8878 - val_loss: 0.3143 - val_accuracy: 0.8977\n",
      "Epoch 16/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3290 - accuracy: 0.8876 - val_loss: 0.3070 - val_accuracy: 0.8980\n",
      "Epoch 17/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3263 - accuracy: 0.8876 - val_loss: 0.3022 - val_accuracy: 0.8971\n",
      "Epoch 18/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3252 - accuracy: 0.8882 - val_loss: 0.3054 - val_accuracy: 0.8967\n",
      "Epoch 19/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3219 - accuracy: 0.8884 - val_loss: 0.3024 - val_accuracy: 0.8979\n",
      "Epoch 20/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3260 - accuracy: 0.8881 - val_loss: 0.3011 - val_accuracy: 0.8973\n",
      "Epoch 21/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3209 - accuracy: 0.8903 - val_loss: 0.3032 - val_accuracy: 0.8983\n",
      "Epoch 22/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3215 - accuracy: 0.8887 - val_loss: 0.3031 - val_accuracy: 0.8979\n",
      "Epoch 23/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3178 - accuracy: 0.8905 - val_loss: 0.3052 - val_accuracy: 0.8980\n",
      "Epoch 24/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3198 - accuracy: 0.8901 - val_loss: 0.2992 - val_accuracy: 0.9008\n",
      "Epoch 25/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3171 - accuracy: 0.8891 - val_loss: 0.3066 - val_accuracy: 0.8980\n",
      "Epoch 26/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3159 - accuracy: 0.8896 - val_loss: 0.2987 - val_accuracy: 0.8985\n",
      "Epoch 27/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3135 - accuracy: 0.8897 - val_loss: 0.3005 - val_accuracy: 0.8983\n",
      "Epoch 28/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3164 - accuracy: 0.8914 - val_loss: 0.3004 - val_accuracy: 0.8982\n",
      "Epoch 29/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3132 - accuracy: 0.8909 - val_loss: 0.2981 - val_accuracy: 0.8985\n",
      "Epoch 30/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3128 - accuracy: 0.8901 - val_loss: 0.2998 - val_accuracy: 0.8970\n",
      "Epoch 31/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3079 - accuracy: 0.8884 - val_loss: 0.2950 - val_accuracy: 0.8990\n",
      "Epoch 32/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3044 - accuracy: 0.8928 - val_loss: 0.2991 - val_accuracy: 0.9008\n",
      "Epoch 33/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3097 - accuracy: 0.8911 - val_loss: 0.2995 - val_accuracy: 0.8990\n",
      "Epoch 34/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3073 - accuracy: 0.8931 - val_loss: 0.2949 - val_accuracy: 0.9007\n",
      "Epoch 35/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3076 - accuracy: 0.8910 - val_loss: 0.2934 - val_accuracy: 0.8993\n",
      "Epoch 36/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3048 - accuracy: 0.8908 - val_loss: 0.2959 - val_accuracy: 0.9004\n",
      "Epoch 37/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3063 - accuracy: 0.8920 - val_loss: 0.2957 - val_accuracy: 0.8996\n",
      "Epoch 38/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3041 - accuracy: 0.8916 - val_loss: 0.2912 - val_accuracy: 0.9007\n",
      "Epoch 39/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3033 - accuracy: 0.8926 - val_loss: 0.2940 - val_accuracy: 0.8995\n",
      "Epoch 40/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3039 - accuracy: 0.8912 - val_loss: 0.2886 - val_accuracy: 0.8990\n",
      "Epoch 41/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3025 - accuracy: 0.8935 - val_loss: 0.2938 - val_accuracy: 0.8983\n",
      "Epoch 42/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3053 - accuracy: 0.8924 - val_loss: 0.2946 - val_accuracy: 0.8977\n",
      "Epoch 43/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2976 - accuracy: 0.8936 - val_loss: 0.2917 - val_accuracy: 0.9004\n",
      "Epoch 44/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2966 - accuracy: 0.8931 - val_loss: 0.2887 - val_accuracy: 0.9014\n",
      "Epoch 45/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3004 - accuracy: 0.8939 - val_loss: 0.2892 - val_accuracy: 0.8992\n",
      "Epoch 46/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2973 - accuracy: 0.8946 - val_loss: 0.2848 - val_accuracy: 0.8989\n",
      "Epoch 47/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2939 - accuracy: 0.8946 - val_loss: 0.2883 - val_accuracy: 0.8990\n",
      "Epoch 48/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2967 - accuracy: 0.8928 - val_loss: 0.2864 - val_accuracy: 0.8998\n",
      "Epoch 49/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2876 - accuracy: 0.8961 - val_loss: 0.2877 - val_accuracy: 0.9002\n",
      "Epoch 50/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2925 - accuracy: 0.8968 - val_loss: 0.2842 - val_accuracy: 0.9005\n",
      "Epoch 51/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2905 - accuracy: 0.8952 - val_loss: 0.2814 - val_accuracy: 0.9024\n",
      "Epoch 52/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2874 - accuracy: 0.8949 - val_loss: 0.2819 - val_accuracy: 0.9023\n",
      "Epoch 53/300\n",
      "212/212 [==============================] - 1s 5ms/step - loss: 0.2887 - accuracy: 0.8962 - val_loss: 0.2871 - val_accuracy: 0.9023\n",
      "Epoch 54/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2915 - accuracy: 0.8959 - val_loss: 0.2817 - val_accuracy: 0.9023\n",
      "Epoch 55/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2862 - accuracy: 0.8983 - val_loss: 0.2825 - val_accuracy: 0.9021\n",
      "Epoch 56/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2862 - accuracy: 0.8967 - val_loss: 0.2823 - val_accuracy: 0.9032\n",
      "Epoch 57/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2820 - accuracy: 0.8969 - val_loss: 0.2798 - val_accuracy: 0.9038\n",
      "Epoch 58/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2841 - accuracy: 0.8968 - val_loss: 0.2805 - val_accuracy: 0.9029\n",
      "Epoch 59/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2856 - accuracy: 0.8967 - val_loss: 0.2828 - val_accuracy: 0.9023\n",
      "Epoch 60/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2821 - accuracy: 0.8976 - val_loss: 0.2806 - val_accuracy: 0.9014\n",
      "Epoch 61/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2821 - accuracy: 0.8982 - val_loss: 0.2807 - val_accuracy: 0.9002\n",
      "Epoch 62/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2783 - accuracy: 0.8999 - val_loss: 0.2814 - val_accuracy: 0.9021\n",
      "Epoch 63/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2801 - accuracy: 0.8993 - val_loss: 0.2750 - val_accuracy: 0.9026\n",
      "Epoch 64/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2778 - accuracy: 0.8995 - val_loss: 0.2758 - val_accuracy: 0.9042\n",
      "Epoch 65/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2757 - accuracy: 0.9003 - val_loss: 0.2814 - val_accuracy: 0.9017\n",
      "Epoch 66/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2786 - accuracy: 0.9017 - val_loss: 0.2806 - val_accuracy: 0.9032\n",
      "Epoch 67/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2818 - accuracy: 0.8995 - val_loss: 0.2801 - val_accuracy: 0.9063\n",
      "Epoch 68/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2755 - accuracy: 0.8995 - val_loss: 0.2746 - val_accuracy: 0.9039\n",
      "Epoch 69/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2735 - accuracy: 0.9006 - val_loss: 0.2814 - val_accuracy: 0.9007\n",
      "Epoch 70/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2747 - accuracy: 0.9009 - val_loss: 0.2751 - val_accuracy: 0.9054\n",
      "Epoch 71/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2774 - accuracy: 0.8999 - val_loss: 0.2761 - val_accuracy: 0.9020\n",
      "Epoch 72/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2687 - accuracy: 0.9021 - val_loss: 0.2768 - val_accuracy: 0.9035\n",
      "Epoch 73/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2725 - accuracy: 0.9011 - val_loss: 0.2767 - val_accuracy: 0.9048\n",
      "Epoch 74/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2708 - accuracy: 0.9014 - val_loss: 0.2850 - val_accuracy: 0.9010\n",
      "Epoch 75/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2723 - accuracy: 0.9020 - val_loss: 0.2766 - val_accuracy: 0.9064\n",
      "Epoch 76/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2673 - accuracy: 0.9028 - val_loss: 0.2829 - val_accuracy: 0.9038\n",
      "Epoch 77/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2701 - accuracy: 0.9013 - val_loss: 0.2792 - val_accuracy: 0.9035\n",
      "Epoch 78/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2714 - accuracy: 0.9021 - val_loss: 0.2798 - val_accuracy: 0.9027\n",
      "Epoch 79/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2731 - accuracy: 0.9006 - val_loss: 0.2784 - val_accuracy: 0.9017\n",
      "Epoch 80/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2679 - accuracy: 0.9034 - val_loss: 0.2784 - val_accuracy: 0.9017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2670 - accuracy: 0.9020 - val_loss: 0.2792 - val_accuracy: 0.9035\n",
      "Epoch 82/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2669 - accuracy: 0.9024 - val_loss: 0.2802 - val_accuracy: 0.9013\n",
      "Epoch 83/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2666 - accuracy: 0.9022 - val_loss: 0.2786 - val_accuracy: 0.9017\n",
      "Epoch 84/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2654 - accuracy: 0.9029 - val_loss: 0.2793 - val_accuracy: 0.8995\n",
      "Epoch 85/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2636 - accuracy: 0.9024 - val_loss: 0.2783 - val_accuracy: 0.9015\n",
      "Epoch 86/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2658 - accuracy: 0.9020 - val_loss: 0.2799 - val_accuracy: 0.9018\n",
      "Epoch 87/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2639 - accuracy: 0.9028 - val_loss: 0.2754 - val_accuracy: 0.9027\n",
      "Epoch 88/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2643 - accuracy: 0.9038 - val_loss: 0.2760 - val_accuracy: 0.9015\n",
      "Epoch 1/300\n",
      "212/212 [==============================] - 1s 5ms/step - loss: 0.4886 - accuracy: 0.8604 - val_loss: 0.3626 - val_accuracy: 0.8837\n",
      "Epoch 2/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3925 - accuracy: 0.8763 - val_loss: 0.3484 - val_accuracy: 0.8834\n",
      "Epoch 3/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3720 - accuracy: 0.8803 - val_loss: 0.3392 - val_accuracy: 0.8859\n",
      "Epoch 4/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3667 - accuracy: 0.8806 - val_loss: 0.3389 - val_accuracy: 0.8866\n",
      "Epoch 5/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3574 - accuracy: 0.8822 - val_loss: 0.3332 - val_accuracy: 0.8902\n",
      "Epoch 6/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3536 - accuracy: 0.8824 - val_loss: 0.3284 - val_accuracy: 0.8896\n",
      "Epoch 7/300\n",
      "212/212 [==============================] - 1s 5ms/step - loss: 0.3474 - accuracy: 0.8840 - val_loss: 0.3274 - val_accuracy: 0.8912\n",
      "Epoch 8/300\n",
      "212/212 [==============================] - 1s 5ms/step - loss: 0.3437 - accuracy: 0.8850 - val_loss: 0.3252 - val_accuracy: 0.8930\n",
      "Epoch 9/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3405 - accuracy: 0.8855 - val_loss: 0.3224 - val_accuracy: 0.8923\n",
      "Epoch 10/300\n",
      "212/212 [==============================] - 1s 5ms/step - loss: 0.3372 - accuracy: 0.8872 - val_loss: 0.3236 - val_accuracy: 0.8918\n",
      "Epoch 11/300\n",
      "212/212 [==============================] - 1s 5ms/step - loss: 0.3376 - accuracy: 0.8873 - val_loss: 0.3182 - val_accuracy: 0.8915\n",
      "Epoch 12/300\n",
      "212/212 [==============================] - 1s 5ms/step - loss: 0.3345 - accuracy: 0.8885 - val_loss: 0.3182 - val_accuracy: 0.8915\n",
      "Epoch 13/300\n",
      "212/212 [==============================] - 1s 5ms/step - loss: 0.3311 - accuracy: 0.8893 - val_loss: 0.3157 - val_accuracy: 0.8928\n",
      "Epoch 14/300\n",
      "212/212 [==============================] - 1s 5ms/step - loss: 0.3304 - accuracy: 0.8889 - val_loss: 0.3156 - val_accuracy: 0.8940\n",
      "Epoch 15/300\n",
      "212/212 [==============================] - 1s 5ms/step - loss: 0.3239 - accuracy: 0.8888 - val_loss: 0.3182 - val_accuracy: 0.8939\n",
      "Epoch 16/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3257 - accuracy: 0.8881 - val_loss: 0.3145 - val_accuracy: 0.8948\n",
      "Epoch 17/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3238 - accuracy: 0.8892 - val_loss: 0.3164 - val_accuracy: 0.8946\n",
      "Epoch 18/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3224 - accuracy: 0.8905 - val_loss: 0.3144 - val_accuracy: 0.8921\n",
      "Epoch 19/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3193 - accuracy: 0.8892 - val_loss: 0.3126 - val_accuracy: 0.8939\n",
      "Epoch 20/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3208 - accuracy: 0.8905 - val_loss: 0.3081 - val_accuracy: 0.8937\n",
      "Epoch 21/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3203 - accuracy: 0.8906 - val_loss: 0.3108 - val_accuracy: 0.8949\n",
      "Epoch 22/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3182 - accuracy: 0.8916 - val_loss: 0.3106 - val_accuracy: 0.8934\n",
      "Epoch 23/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3193 - accuracy: 0.8907 - val_loss: 0.3109 - val_accuracy: 0.8942\n",
      "Epoch 24/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3155 - accuracy: 0.8929 - val_loss: 0.3084 - val_accuracy: 0.8937\n",
      "Epoch 25/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3134 - accuracy: 0.8924 - val_loss: 0.3082 - val_accuracy: 0.8933\n",
      "Epoch 26/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3155 - accuracy: 0.8912 - val_loss: 0.3069 - val_accuracy: 0.8933\n",
      "Epoch 27/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3129 - accuracy: 0.8919 - val_loss: 0.3063 - val_accuracy: 0.8928\n",
      "Epoch 28/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3119 - accuracy: 0.8913 - val_loss: 0.3056 - val_accuracy: 0.8933\n",
      "Epoch 29/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3123 - accuracy: 0.8929 - val_loss: 0.3066 - val_accuracy: 0.8934\n",
      "Epoch 30/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3095 - accuracy: 0.8916 - val_loss: 0.3028 - val_accuracy: 0.8931\n",
      "Epoch 31/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3075 - accuracy: 0.8942 - val_loss: 0.3084 - val_accuracy: 0.8937\n",
      "Epoch 32/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3049 - accuracy: 0.8928 - val_loss: 0.3085 - val_accuracy: 0.8954\n",
      "Epoch 33/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3061 - accuracy: 0.8939 - val_loss: 0.3014 - val_accuracy: 0.8943\n",
      "Epoch 34/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3060 - accuracy: 0.8935 - val_loss: 0.3021 - val_accuracy: 0.8934\n",
      "Epoch 35/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3034 - accuracy: 0.8938 - val_loss: 0.3060 - val_accuracy: 0.8928\n",
      "Epoch 36/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3033 - accuracy: 0.8934 - val_loss: 0.3044 - val_accuracy: 0.8924\n",
      "Epoch 37/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2995 - accuracy: 0.8950 - val_loss: 0.3013 - val_accuracy: 0.8939\n",
      "Epoch 38/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3004 - accuracy: 0.8938 - val_loss: 0.2996 - val_accuracy: 0.8949\n",
      "Epoch 39/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3011 - accuracy: 0.8943 - val_loss: 0.3005 - val_accuracy: 0.8931\n",
      "Epoch 40/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2968 - accuracy: 0.8942 - val_loss: 0.2966 - val_accuracy: 0.8954\n",
      "Epoch 41/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2982 - accuracy: 0.8962 - val_loss: 0.2967 - val_accuracy: 0.8946\n",
      "Epoch 42/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2968 - accuracy: 0.8983 - val_loss: 0.2958 - val_accuracy: 0.8931\n",
      "Epoch 43/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2922 - accuracy: 0.8942 - val_loss: 0.2952 - val_accuracy: 0.8943\n",
      "Epoch 44/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2958 - accuracy: 0.8940 - val_loss: 0.2921 - val_accuracy: 0.8961\n",
      "Epoch 45/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2900 - accuracy: 0.8966 - val_loss: 0.2952 - val_accuracy: 0.8954\n",
      "Epoch 46/300\n",
      "212/212 [==============================] - ETA: 0s - loss: 0.2929 - accuracy: 0.89 - 1s 4ms/step - loss: 0.2938 - accuracy: 0.8955 - val_loss: 0.2962 - val_accuracy: 0.8958\n",
      "Epoch 47/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2884 - accuracy: 0.8966 - val_loss: 0.2902 - val_accuracy: 0.8949\n",
      "Epoch 48/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2880 - accuracy: 0.8980 - val_loss: 0.2911 - val_accuracy: 0.8954\n",
      "Epoch 49/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2884 - accuracy: 0.8985 - val_loss: 0.2890 - val_accuracy: 0.8956\n",
      "Epoch 50/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2850 - accuracy: 0.8975 - val_loss: 0.2887 - val_accuracy: 0.8955\n",
      "Epoch 51/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2886 - accuracy: 0.9003 - val_loss: 0.2902 - val_accuracy: 0.8971\n",
      "Epoch 52/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2855 - accuracy: 0.8991 - val_loss: 0.2849 - val_accuracy: 0.8954\n",
      "Epoch 53/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2807 - accuracy: 0.8992 - val_loss: 0.2849 - val_accuracy: 0.8970\n",
      "Epoch 54/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2824 - accuracy: 0.8990 - val_loss: 0.2857 - val_accuracy: 0.8958\n",
      "Epoch 55/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2827 - accuracy: 0.9002 - val_loss: 0.2915 - val_accuracy: 0.8976\n",
      "Epoch 56/300\n",
      "212/212 [==============================] - 1s 5ms/step - loss: 0.2782 - accuracy: 0.9004 - val_loss: 0.2879 - val_accuracy: 0.8973\n",
      "Epoch 57/300\n",
      "212/212 [==============================] - 1s 5ms/step - loss: 0.2793 - accuracy: 0.9001 - val_loss: 0.2871 - val_accuracy: 0.8965\n",
      "Epoch 58/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2806 - accuracy: 0.9012 - val_loss: 0.2838 - val_accuracy: 0.8976\n",
      "Epoch 59/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2783 - accuracy: 0.9020 - val_loss: 0.2875 - val_accuracy: 0.8967\n",
      "Epoch 60/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2785 - accuracy: 0.9017 - val_loss: 0.2830 - val_accuracy: 0.8974\n",
      "Epoch 61/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2759 - accuracy: 0.9012 - val_loss: 0.2841 - val_accuracy: 0.8980\n",
      "Epoch 62/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2739 - accuracy: 0.9022 - val_loss: 0.2845 - val_accuracy: 0.8979\n",
      "Epoch 63/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2714 - accuracy: 0.9022 - val_loss: 0.2848 - val_accuracy: 0.8977\n",
      "Epoch 64/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2715 - accuracy: 0.9040 - val_loss: 0.2833 - val_accuracy: 0.8968\n",
      "Epoch 65/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2744 - accuracy: 0.9030 - val_loss: 0.2875 - val_accuracy: 0.8959\n",
      "Epoch 66/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2712 - accuracy: 0.9020 - val_loss: 0.2852 - val_accuracy: 0.8970\n",
      "Epoch 67/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2706 - accuracy: 0.9028 - val_loss: 0.2877 - val_accuracy: 0.8967\n",
      "Epoch 68/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2712 - accuracy: 0.9043 - val_loss: 0.2846 - val_accuracy: 0.8976\n",
      "Epoch 69/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2715 - accuracy: 0.9040 - val_loss: 0.2833 - val_accuracy: 0.8959\n",
      "Epoch 70/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2716 - accuracy: 0.9037 - val_loss: 0.2842 - val_accuracy: 0.8979\n",
      "Epoch 71/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2708 - accuracy: 0.9038 - val_loss: 0.2847 - val_accuracy: 0.8968\n",
      "Epoch 72/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2677 - accuracy: 0.9036 - val_loss: 0.2852 - val_accuracy: 0.8970\n",
      "Epoch 73/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2684 - accuracy: 0.9034 - val_loss: 0.2865 - val_accuracy: 0.8977\n",
      "Epoch 74/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2654 - accuracy: 0.9048 - val_loss: 0.2863 - val_accuracy: 0.8952\n",
      "Epoch 75/300\n",
      "212/212 [==============================] - 1s 5ms/step - loss: 0.2708 - accuracy: 0.9024 - val_loss: 0.2865 - val_accuracy: 0.8973\n",
      "Epoch 76/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2691 - accuracy: 0.9057 - val_loss: 0.2870 - val_accuracy: 0.8965\n",
      "Epoch 77/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2668 - accuracy: 0.9066 - val_loss: 0.2884 - val_accuracy: 0.8964\n",
      "Epoch 78/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2654 - accuracy: 0.9042 - val_loss: 0.2896 - val_accuracy: 0.8956\n",
      "Epoch 79/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2642 - accuracy: 0.9032 - val_loss: 0.2925 - val_accuracy: 0.8959\n",
      "Epoch 80/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2644 - accuracy: 0.9064 - val_loss: 0.2871 - val_accuracy: 0.8967\n",
      "Epoch 1/300\n",
      "212/212 [==============================] - 1s 5ms/step - loss: 0.5111 - accuracy: 0.8457 - val_loss: 0.3613 - val_accuracy: 0.8787\n",
      "Epoch 2/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3885 - accuracy: 0.8764 - val_loss: 0.3482 - val_accuracy: 0.8825\n",
      "Epoch 3/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3734 - accuracy: 0.8786 - val_loss: 0.3440 - val_accuracy: 0.8827\n",
      "Epoch 4/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3620 - accuracy: 0.8829 - val_loss: 0.3352 - val_accuracy: 0.8846\n",
      "Epoch 5/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3596 - accuracy: 0.8822 - val_loss: 0.3369 - val_accuracy: 0.8843\n",
      "Epoch 6/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3480 - accuracy: 0.8854 - val_loss: 0.3320 - val_accuracy: 0.8865\n",
      "Epoch 7/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3447 - accuracy: 0.8866 - val_loss: 0.3317 - val_accuracy: 0.8874\n",
      "Epoch 8/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3406 - accuracy: 0.8865 - val_loss: 0.3274 - val_accuracy: 0.8883\n",
      "Epoch 9/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3333 - accuracy: 0.8875 - val_loss: 0.3242 - val_accuracy: 0.8880\n",
      "Epoch 10/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3360 - accuracy: 0.8872 - val_loss: 0.3234 - val_accuracy: 0.8890\n",
      "Epoch 11/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3318 - accuracy: 0.8884 - val_loss: 0.3219 - val_accuracy: 0.8897\n",
      "Epoch 12/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3299 - accuracy: 0.8900 - val_loss: 0.3256 - val_accuracy: 0.8890\n",
      "Epoch 13/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3291 - accuracy: 0.8887 - val_loss: 0.3190 - val_accuracy: 0.8887\n",
      "Epoch 14/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3264 - accuracy: 0.8893 - val_loss: 0.3213 - val_accuracy: 0.8889\n",
      "Epoch 15/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3253 - accuracy: 0.8883 - val_loss: 0.3211 - val_accuracy: 0.8900\n",
      "Epoch 16/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3248 - accuracy: 0.8892 - val_loss: 0.3222 - val_accuracy: 0.8897\n",
      "Epoch 17/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3224 - accuracy: 0.8883 - val_loss: 0.3191 - val_accuracy: 0.8896\n",
      "Epoch 18/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3213 - accuracy: 0.8894 - val_loss: 0.3175 - val_accuracy: 0.8893\n",
      "Epoch 19/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3177 - accuracy: 0.8907 - val_loss: 0.3199 - val_accuracy: 0.8912\n",
      "Epoch 20/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3182 - accuracy: 0.8901 - val_loss: 0.3203 - val_accuracy: 0.8893\n",
      "Epoch 21/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3168 - accuracy: 0.8919 - val_loss: 0.3162 - val_accuracy: 0.8900\n",
      "Epoch 22/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3163 - accuracy: 0.8898 - val_loss: 0.3172 - val_accuracy: 0.8896\n",
      "Epoch 23/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3152 - accuracy: 0.8916 - val_loss: 0.3163 - val_accuracy: 0.8893\n",
      "Epoch 24/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3128 - accuracy: 0.8913 - val_loss: 0.3150 - val_accuracy: 0.8893\n",
      "Epoch 25/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3113 - accuracy: 0.8922 - val_loss: 0.3163 - val_accuracy: 0.8899\n",
      "Epoch 26/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3095 - accuracy: 0.8938 - val_loss: 0.3145 - val_accuracy: 0.8900\n",
      "Epoch 27/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3082 - accuracy: 0.8920 - val_loss: 0.3138 - val_accuracy: 0.8897\n",
      "Epoch 28/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3103 - accuracy: 0.8949 - val_loss: 0.3132 - val_accuracy: 0.8890\n",
      "Epoch 29/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3096 - accuracy: 0.8922 - val_loss: 0.3125 - val_accuracy: 0.8894\n",
      "Epoch 30/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3080 - accuracy: 0.8921 - val_loss: 0.3136 - val_accuracy: 0.8897\n",
      "Epoch 31/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3087 - accuracy: 0.8934 - val_loss: 0.3126 - val_accuracy: 0.8902\n",
      "Epoch 32/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3034 - accuracy: 0.8941 - val_loss: 0.3115 - val_accuracy: 0.8899\n",
      "Epoch 33/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3037 - accuracy: 0.8947 - val_loss: 0.3118 - val_accuracy: 0.8884\n",
      "Epoch 34/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3014 - accuracy: 0.8931 - val_loss: 0.3129 - val_accuracy: 0.8899\n",
      "Epoch 35/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3020 - accuracy: 0.8941 - val_loss: 0.3144 - val_accuracy: 0.8905\n",
      "Epoch 36/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2982 - accuracy: 0.8955 - val_loss: 0.3108 - val_accuracy: 0.8900\n",
      "Epoch 37/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2985 - accuracy: 0.8948 - val_loss: 0.3133 - val_accuracy: 0.8908\n",
      "Epoch 38/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.3022 - accuracy: 0.8930 - val_loss: 0.3108 - val_accuracy: 0.8897\n",
      "Epoch 39/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2984 - accuracy: 0.8954 - val_loss: 0.3096 - val_accuracy: 0.8914\n",
      "Epoch 40/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2957 - accuracy: 0.8951 - val_loss: 0.3084 - val_accuracy: 0.8897\n",
      "Epoch 41/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2956 - accuracy: 0.8945 - val_loss: 0.3107 - val_accuracy: 0.8920\n",
      "Epoch 42/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2956 - accuracy: 0.8958 - val_loss: 0.3091 - val_accuracy: 0.8906\n",
      "Epoch 43/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2933 - accuracy: 0.8964 - val_loss: 0.3106 - val_accuracy: 0.8890\n",
      "Epoch 44/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2954 - accuracy: 0.8955 - val_loss: 0.3082 - val_accuracy: 0.8894\n",
      "Epoch 45/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2933 - accuracy: 0.8961 - val_loss: 0.3076 - val_accuracy: 0.8906\n",
      "Epoch 46/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2948 - accuracy: 0.8947 - val_loss: 0.3084 - val_accuracy: 0.8909\n",
      "Epoch 47/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2919 - accuracy: 0.8974 - val_loss: 0.3058 - val_accuracy: 0.8902\n",
      "Epoch 48/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2904 - accuracy: 0.8968 - val_loss: 0.3060 - val_accuracy: 0.8903\n",
      "Epoch 49/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2903 - accuracy: 0.8970 - val_loss: 0.3052 - val_accuracy: 0.8902\n",
      "Epoch 50/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2883 - accuracy: 0.8971 - val_loss: 0.3041 - val_accuracy: 0.8912\n",
      "Epoch 51/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2898 - accuracy: 0.8985 - val_loss: 0.3045 - val_accuracy: 0.8914\n",
      "Epoch 52/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2820 - accuracy: 0.8993 - val_loss: 0.3028 - val_accuracy: 0.8918\n",
      "Epoch 53/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2860 - accuracy: 0.8964 - val_loss: 0.3024 - val_accuracy: 0.8918\n",
      "Epoch 54/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2905 - accuracy: 0.8956 - val_loss: 0.3076 - val_accuracy: 0.8917\n",
      "Epoch 55/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2784 - accuracy: 0.8989 - val_loss: 0.3046 - val_accuracy: 0.8906\n",
      "Epoch 56/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2841 - accuracy: 0.8984 - val_loss: 0.3014 - val_accuracy: 0.8906\n",
      "Epoch 57/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2840 - accuracy: 0.8973 - val_loss: 0.3039 - val_accuracy: 0.8924\n",
      "Epoch 58/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2836 - accuracy: 0.8989 - val_loss: 0.3031 - val_accuracy: 0.8925\n",
      "Epoch 59/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2804 - accuracy: 0.8995 - val_loss: 0.3024 - val_accuracy: 0.8931\n",
      "Epoch 60/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2800 - accuracy: 0.8985 - val_loss: 0.3026 - val_accuracy: 0.8920\n",
      "Epoch 61/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2787 - accuracy: 0.9001 - val_loss: 0.2981 - val_accuracy: 0.8946\n",
      "Epoch 62/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2835 - accuracy: 0.8998 - val_loss: 0.2968 - val_accuracy: 0.8939\n",
      "Epoch 63/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2772 - accuracy: 0.9011 - val_loss: 0.3004 - val_accuracy: 0.8911\n",
      "Epoch 64/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2781 - accuracy: 0.9006 - val_loss: 0.3021 - val_accuracy: 0.8920\n",
      "Epoch 65/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2763 - accuracy: 0.9003 - val_loss: 0.3024 - val_accuracy: 0.8931\n",
      "Epoch 66/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2767 - accuracy: 0.8985 - val_loss: 0.2998 - val_accuracy: 0.8927\n",
      "Epoch 67/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2788 - accuracy: 0.9008 - val_loss: 0.2957 - val_accuracy: 0.8943\n",
      "Epoch 68/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2705 - accuracy: 0.9006 - val_loss: 0.2965 - val_accuracy: 0.8967\n",
      "Epoch 69/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2741 - accuracy: 0.9004 - val_loss: 0.3008 - val_accuracy: 0.8924\n",
      "Epoch 70/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2744 - accuracy: 0.9013 - val_loss: 0.2972 - val_accuracy: 0.8952\n",
      "Epoch 71/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2730 - accuracy: 0.8991 - val_loss: 0.2989 - val_accuracy: 0.8945\n",
      "Epoch 72/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2765 - accuracy: 0.9011 - val_loss: 0.2988 - val_accuracy: 0.8948\n",
      "Epoch 73/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2730 - accuracy: 0.9013 - val_loss: 0.2986 - val_accuracy: 0.8933\n",
      "Epoch 74/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2731 - accuracy: 0.9009 - val_loss: 0.2960 - val_accuracy: 0.8945\n",
      "Epoch 75/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2715 - accuracy: 0.9004 - val_loss: 0.2959 - val_accuracy: 0.8949\n",
      "Epoch 76/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2666 - accuracy: 0.9046 - val_loss: 0.2977 - val_accuracy: 0.8948\n",
      "Epoch 77/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2669 - accuracy: 0.9030 - val_loss: 0.2976 - val_accuracy: 0.8939\n",
      "Epoch 78/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2690 - accuracy: 0.9008 - val_loss: 0.2954 - val_accuracy: 0.8945\n",
      "Epoch 79/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2634 - accuracy: 0.9037 - val_loss: 0.2936 - val_accuracy: 0.8967\n",
      "Epoch 80/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2692 - accuracy: 0.9010 - val_loss: 0.2935 - val_accuracy: 0.8951\n",
      "Epoch 81/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2660 - accuracy: 0.9025 - val_loss: 0.2924 - val_accuracy: 0.8974\n",
      "Epoch 82/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2653 - accuracy: 0.9037 - val_loss: 0.2956 - val_accuracy: 0.8936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2675 - accuracy: 0.9028 - val_loss: 0.2972 - val_accuracy: 0.8940\n",
      "Epoch 84/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2642 - accuracy: 0.9029 - val_loss: 0.2950 - val_accuracy: 0.8948\n",
      "Epoch 85/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2638 - accuracy: 0.9022 - val_loss: 0.2968 - val_accuracy: 0.8964\n",
      "Epoch 86/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2668 - accuracy: 0.9031 - val_loss: 0.2930 - val_accuracy: 0.8959\n",
      "Epoch 87/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2623 - accuracy: 0.9039 - val_loss: 0.2948 - val_accuracy: 0.8959\n",
      "Epoch 88/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2669 - accuracy: 0.9024 - val_loss: 0.2941 - val_accuracy: 0.8967\n",
      "Epoch 89/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2641 - accuracy: 0.9068 - val_loss: 0.2924 - val_accuracy: 0.8965\n",
      "Epoch 90/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2627 - accuracy: 0.9037 - val_loss: 0.2946 - val_accuracy: 0.8949\n",
      "Epoch 91/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2648 - accuracy: 0.9038 - val_loss: 0.2899 - val_accuracy: 0.8970\n",
      "Epoch 92/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2619 - accuracy: 0.9046 - val_loss: 0.2907 - val_accuracy: 0.8982\n",
      "Epoch 93/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2643 - accuracy: 0.9037 - val_loss: 0.2963 - val_accuracy: 0.8958\n",
      "Epoch 94/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2632 - accuracy: 0.9057 - val_loss: 0.2923 - val_accuracy: 0.8965\n",
      "Epoch 95/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2629 - accuracy: 0.9042 - val_loss: 0.2941 - val_accuracy: 0.8958\n",
      "Epoch 96/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2582 - accuracy: 0.9061 - val_loss: 0.2967 - val_accuracy: 0.8958\n",
      "Epoch 97/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2590 - accuracy: 0.9039 - val_loss: 0.2955 - val_accuracy: 0.8952\n",
      "Epoch 98/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2586 - accuracy: 0.9048 - val_loss: 0.2953 - val_accuracy: 0.8967\n",
      "Epoch 99/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2584 - accuracy: 0.9054 - val_loss: 0.2932 - val_accuracy: 0.8961\n",
      "Epoch 100/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2594 - accuracy: 0.9062 - val_loss: 0.2929 - val_accuracy: 0.8952\n",
      "Epoch 101/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2561 - accuracy: 0.9070 - val_loss: 0.2933 - val_accuracy: 0.8970\n",
      "Epoch 102/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2612 - accuracy: 0.9076 - val_loss: 0.2928 - val_accuracy: 0.8958\n",
      "Epoch 103/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2569 - accuracy: 0.9054 - val_loss: 0.2914 - val_accuracy: 0.8962\n",
      "Epoch 104/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2524 - accuracy: 0.9049 - val_loss: 0.2911 - val_accuracy: 0.8949\n",
      "Epoch 105/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2596 - accuracy: 0.9058 - val_loss: 0.2927 - val_accuracy: 0.8967\n",
      "Epoch 106/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2605 - accuracy: 0.9046 - val_loss: 0.2930 - val_accuracy: 0.8964\n",
      "Epoch 107/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2523 - accuracy: 0.9064 - val_loss: 0.2953 - val_accuracy: 0.8958\n",
      "Epoch 108/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2535 - accuracy: 0.9077 - val_loss: 0.2986 - val_accuracy: 0.8956\n",
      "Epoch 109/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2543 - accuracy: 0.9071 - val_loss: 0.2968 - val_accuracy: 0.8964\n",
      "Epoch 110/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2582 - accuracy: 0.9051 - val_loss: 0.2952 - val_accuracy: 0.8946\n",
      "Epoch 111/300\n",
      "212/212 [==============================] - 1s 4ms/step - loss: 0.2501 - accuracy: 0.9072 - val_loss: 0.2911 - val_accuracy: 0.8980\n",
      "[11:12:48] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-merror:0.08177\teval-merror:0.08723\n",
      "Multiple eval metrics have been passed: 'eval-merror' will be used for early stopping.\n",
      "\n",
      "Will train until eval-merror hasn't improved in 20 rounds.\n",
      "[1]\ttrain-merror:0.07877\teval-merror:0.08635\n",
      "[2]\ttrain-merror:0.07705\teval-merror:0.08576\n",
      "[3]\ttrain-merror:0.07680\teval-merror:0.08561\n",
      "[4]\ttrain-merror:0.07793\teval-merror:0.08797\n",
      "[5]\ttrain-merror:0.07705\teval-merror:0.08782\n",
      "[6]\ttrain-merror:0.07749\teval-merror:0.08723\n",
      "[7]\ttrain-merror:0.07680\teval-merror:0.08768\n",
      "[8]\ttrain-merror:0.07641\teval-merror:0.08723\n",
      "[9]\ttrain-merror:0.07621\teval-merror:0.08664\n",
      "[10]\ttrain-merror:0.07183\teval-merror:0.08325\n",
      "[11]\ttrain-merror:0.07149\teval-merror:0.08325\n",
      "[12]\ttrain-merror:0.07149\teval-merror:0.08266\n",
      "[13]\ttrain-merror:0.07090\teval-merror:0.08384\n",
      "[14]\ttrain-merror:0.07065\teval-merror:0.08399\n",
      "[15]\ttrain-merror:0.07001\teval-merror:0.08399\n",
      "[16]\ttrain-merror:0.07001\teval-merror:0.08354\n",
      "[17]\ttrain-merror:0.07001\teval-merror:0.08325\n",
      "[18]\ttrain-merror:0.06986\teval-merror:0.08325\n",
      "[19]\ttrain-merror:0.06952\teval-merror:0.08266\n",
      "[20]\ttrain-merror:0.06908\teval-merror:0.08251\n",
      "[21]\ttrain-merror:0.06849\teval-merror:0.08236\n",
      "[22]\ttrain-merror:0.06834\teval-merror:0.08192\n",
      "[23]\ttrain-merror:0.06750\teval-merror:0.08162\n",
      "[24]\ttrain-merror:0.06741\teval-merror:0.08177\n",
      "[25]\ttrain-merror:0.06726\teval-merror:0.08192\n",
      "[26]\ttrain-merror:0.06657\teval-merror:0.08133\n",
      "[27]\ttrain-merror:0.06622\teval-merror:0.08133\n",
      "[28]\ttrain-merror:0.06583\teval-merror:0.08148\n",
      "[29]\ttrain-merror:0.06544\teval-merror:0.08162\n",
      "[30]\ttrain-merror:0.06519\teval-merror:0.08133\n",
      "[31]\ttrain-merror:0.06489\teval-merror:0.08148\n",
      "[32]\ttrain-merror:0.06426\teval-merror:0.08089\n",
      "[33]\ttrain-merror:0.06455\teval-merror:0.08015\n",
      "[34]\ttrain-merror:0.06411\teval-merror:0.08044\n",
      "[35]\ttrain-merror:0.06342\teval-merror:0.08059\n",
      "[36]\ttrain-merror:0.06308\teval-merror:0.08044\n",
      "[37]\ttrain-merror:0.06293\teval-merror:0.08030\n",
      "[38]\ttrain-merror:0.06273\teval-merror:0.07926\n",
      "[39]\ttrain-merror:0.06229\teval-merror:0.07956\n",
      "[40]\ttrain-merror:0.06160\teval-merror:0.07941\n",
      "[41]\ttrain-merror:0.06121\teval-merror:0.07926\n",
      "[42]\ttrain-merror:0.06057\teval-merror:0.07882\n",
      "[43]\ttrain-merror:0.06032\teval-merror:0.07838\n",
      "[44]\ttrain-merror:0.06012\teval-merror:0.07838\n",
      "[45]\ttrain-merror:0.05983\teval-merror:0.07852\n",
      "[46]\ttrain-merror:0.05978\teval-merror:0.07838\n",
      "[47]\ttrain-merror:0.05939\teval-merror:0.07911\n",
      "[48]\ttrain-merror:0.05889\teval-merror:0.07867\n",
      "[49]\ttrain-merror:0.05865\teval-merror:0.07838\n",
      "[50]\ttrain-merror:0.05820\teval-merror:0.07808\n",
      "[51]\ttrain-merror:0.05801\teval-merror:0.07793\n",
      "[52]\ttrain-merror:0.05742\teval-merror:0.07779\n",
      "[53]\ttrain-merror:0.05732\teval-merror:0.07779\n",
      "[54]\ttrain-merror:0.05707\teval-merror:0.07764\n",
      "[55]\ttrain-merror:0.05678\teval-merror:0.07764\n",
      "[56]\ttrain-merror:0.05614\teval-merror:0.07720\n",
      "[57]\ttrain-merror:0.05574\teval-merror:0.07720\n",
      "[58]\ttrain-merror:0.05579\teval-merror:0.07749\n",
      "[59]\ttrain-merror:0.05550\teval-merror:0.07749\n",
      "[60]\ttrain-merror:0.05530\teval-merror:0.07720\n",
      "[61]\ttrain-merror:0.05496\teval-merror:0.07720\n",
      "[62]\ttrain-merror:0.05466\teval-merror:0.07720\n",
      "[63]\ttrain-merror:0.05446\teval-merror:0.07734\n",
      "[64]\ttrain-merror:0.05387\teval-merror:0.07779\n",
      "[65]\ttrain-merror:0.05363\teval-merror:0.07764\n",
      "[66]\ttrain-merror:0.05358\teval-merror:0.07690\n",
      "[67]\ttrain-merror:0.05309\teval-merror:0.07705\n",
      "[68]\ttrain-merror:0.05269\teval-merror:0.07720\n",
      "[69]\ttrain-merror:0.05240\teval-merror:0.07690\n",
      "[70]\ttrain-merror:0.05205\teval-merror:0.07749\n",
      "[71]\ttrain-merror:0.05191\teval-merror:0.07734\n",
      "[72]\ttrain-merror:0.05151\teval-merror:0.07779\n",
      "[73]\ttrain-merror:0.05137\teval-merror:0.07779\n",
      "[74]\ttrain-merror:0.05117\teval-merror:0.07720\n",
      "[75]\ttrain-merror:0.05097\teval-merror:0.07779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[76]\ttrain-merror:0.05058\teval-merror:0.07690\n",
      "[77]\ttrain-merror:0.05033\teval-merror:0.07720\n",
      "[78]\ttrain-merror:0.04989\teval-merror:0.07734\n",
      "[79]\ttrain-merror:0.04959\teval-merror:0.07720\n",
      "[80]\ttrain-merror:0.04950\teval-merror:0.07734\n",
      "[81]\ttrain-merror:0.04930\teval-merror:0.07720\n",
      "[82]\ttrain-merror:0.04890\teval-merror:0.07749\n",
      "[83]\ttrain-merror:0.04836\teval-merror:0.07764\n",
      "[84]\ttrain-merror:0.04787\teval-merror:0.07764\n",
      "[85]\ttrain-merror:0.04763\teval-merror:0.07793\n",
      "[86]\ttrain-merror:0.04743\teval-merror:0.07779\n",
      "Stopping. Best iteration:\n",
      "[66]\ttrain-merror:0.05358\teval-merror:0.07690\n",
      "\n",
      "[11:12:53] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-merror:0.08295\teval-merror:0.08472\n",
      "Multiple eval metrics have been passed: 'eval-merror' will be used for early stopping.\n",
      "\n",
      "Will train until eval-merror hasn't improved in 20 rounds.\n",
      "[1]\ttrain-merror:0.08162\teval-merror:0.08458\n",
      "[2]\ttrain-merror:0.08138\teval-merror:0.08413\n",
      "[3]\ttrain-merror:0.08148\teval-merror:0.08369\n",
      "[4]\ttrain-merror:0.07980\teval-merror:0.08354\n",
      "[5]\ttrain-merror:0.07956\teval-merror:0.08177\n",
      "[6]\ttrain-merror:0.07951\teval-merror:0.08339\n",
      "[7]\ttrain-merror:0.07902\teval-merror:0.08266\n",
      "[8]\ttrain-merror:0.07680\teval-merror:0.08251\n",
      "[9]\ttrain-merror:0.07700\teval-merror:0.08133\n",
      "[10]\ttrain-merror:0.07547\teval-merror:0.07970\n",
      "[11]\ttrain-merror:0.07533\teval-merror:0.08044\n",
      "[12]\ttrain-merror:0.07493\teval-merror:0.08074\n",
      "[13]\ttrain-merror:0.07400\teval-merror:0.07897\n",
      "[14]\ttrain-merror:0.07355\teval-merror:0.07911\n",
      "[15]\ttrain-merror:0.07346\teval-merror:0.07897\n",
      "[16]\ttrain-merror:0.07331\teval-merror:0.07867\n",
      "[17]\ttrain-merror:0.07360\teval-merror:0.07852\n",
      "[18]\ttrain-merror:0.07316\teval-merror:0.07734\n",
      "[19]\ttrain-merror:0.07287\teval-merror:0.07705\n",
      "[20]\ttrain-merror:0.07272\teval-merror:0.07675\n",
      "[21]\ttrain-merror:0.07242\teval-merror:0.07720\n",
      "[22]\ttrain-merror:0.07218\teval-merror:0.07720\n",
      "[23]\ttrain-merror:0.07173\teval-merror:0.07661\n",
      "[24]\ttrain-merror:0.07144\teval-merror:0.07661\n",
      "[25]\ttrain-merror:0.07114\teval-merror:0.07616\n",
      "[26]\ttrain-merror:0.07095\teval-merror:0.07572\n",
      "[27]\ttrain-merror:0.07050\teval-merror:0.07587\n",
      "[28]\ttrain-merror:0.07021\teval-merror:0.07557\n",
      "[29]\ttrain-merror:0.06986\teval-merror:0.07601\n",
      "[30]\ttrain-merror:0.06937\teval-merror:0.07601\n",
      "[31]\ttrain-merror:0.06888\teval-merror:0.07587\n",
      "[32]\ttrain-merror:0.06844\teval-merror:0.07513\n",
      "[33]\ttrain-merror:0.06780\teval-merror:0.07483\n",
      "[34]\ttrain-merror:0.06780\teval-merror:0.07498\n",
      "[35]\ttrain-merror:0.06780\teval-merror:0.07483\n",
      "[36]\ttrain-merror:0.06731\teval-merror:0.07513\n",
      "[37]\ttrain-merror:0.06701\teval-merror:0.07454\n",
      "[38]\ttrain-merror:0.06711\teval-merror:0.07454\n",
      "[39]\ttrain-merror:0.06662\teval-merror:0.07410\n",
      "[40]\ttrain-merror:0.06613\teval-merror:0.07351\n",
      "[41]\ttrain-merror:0.06568\teval-merror:0.07351\n",
      "[42]\ttrain-merror:0.06544\teval-merror:0.07365\n",
      "[43]\ttrain-merror:0.06499\teval-merror:0.07380\n",
      "[44]\ttrain-merror:0.06431\teval-merror:0.07365\n",
      "[45]\ttrain-merror:0.06391\teval-merror:0.07365\n",
      "[46]\ttrain-merror:0.06332\teval-merror:0.07336\n",
      "[47]\ttrain-merror:0.06283\teval-merror:0.07306\n",
      "[48]\ttrain-merror:0.06219\teval-merror:0.07291\n",
      "[49]\ttrain-merror:0.06165\teval-merror:0.07291\n",
      "[50]\ttrain-merror:0.06135\teval-merror:0.07203\n",
      "[51]\ttrain-merror:0.06116\teval-merror:0.07188\n",
      "[52]\ttrain-merror:0.06052\teval-merror:0.07188\n",
      "[53]\ttrain-merror:0.06017\teval-merror:0.07159\n",
      "[54]\ttrain-merror:0.06032\teval-merror:0.07159\n",
      "[55]\ttrain-merror:0.05968\teval-merror:0.07188\n",
      "[56]\ttrain-merror:0.05934\teval-merror:0.07173\n",
      "[57]\ttrain-merror:0.05909\teval-merror:0.07129\n",
      "[58]\ttrain-merror:0.05874\teval-merror:0.07114\n",
      "[59]\ttrain-merror:0.05855\teval-merror:0.07100\n",
      "[60]\ttrain-merror:0.05801\teval-merror:0.07085\n",
      "[61]\ttrain-merror:0.05806\teval-merror:0.07085\n",
      "[62]\ttrain-merror:0.05791\teval-merror:0.07070\n",
      "[63]\ttrain-merror:0.05771\teval-merror:0.06996\n",
      "[64]\ttrain-merror:0.05727\teval-merror:0.06996\n",
      "[65]\ttrain-merror:0.05742\teval-merror:0.07026\n",
      "[66]\ttrain-merror:0.05747\teval-merror:0.07011\n",
      "[67]\ttrain-merror:0.05678\teval-merror:0.06996\n",
      "[68]\ttrain-merror:0.05634\teval-merror:0.06952\n",
      "[69]\ttrain-merror:0.05589\teval-merror:0.06952\n",
      "[70]\ttrain-merror:0.05574\teval-merror:0.06937\n",
      "[71]\ttrain-merror:0.05560\teval-merror:0.06908\n",
      "[72]\ttrain-merror:0.05515\teval-merror:0.06893\n",
      "[73]\ttrain-merror:0.05446\teval-merror:0.06893\n",
      "[74]\ttrain-merror:0.05427\teval-merror:0.06922\n",
      "[75]\ttrain-merror:0.05387\teval-merror:0.06952\n",
      "[76]\ttrain-merror:0.05392\teval-merror:0.06967\n",
      "[77]\ttrain-merror:0.05358\teval-merror:0.06952\n",
      "[78]\ttrain-merror:0.05323\teval-merror:0.06952\n",
      "[79]\ttrain-merror:0.05294\teval-merror:0.06982\n",
      "[80]\ttrain-merror:0.05274\teval-merror:0.06996\n",
      "[81]\ttrain-merror:0.05269\teval-merror:0.06967\n",
      "[82]\ttrain-merror:0.05235\teval-merror:0.06937\n",
      "[83]\ttrain-merror:0.05220\teval-merror:0.06922\n",
      "[84]\ttrain-merror:0.05186\teval-merror:0.06967\n",
      "[85]\ttrain-merror:0.05181\teval-merror:0.06982\n",
      "[86]\ttrain-merror:0.05156\teval-merror:0.06996\n",
      "[87]\ttrain-merror:0.05122\teval-merror:0.06996\n",
      "[88]\ttrain-merror:0.05107\teval-merror:0.06952\n",
      "[89]\ttrain-merror:0.05063\teval-merror:0.06952\n",
      "[90]\ttrain-merror:0.05058\teval-merror:0.06967\n",
      "[91]\ttrain-merror:0.05033\teval-merror:0.06967\n",
      "[92]\ttrain-merror:0.05009\teval-merror:0.06967\n",
      "Stopping. Best iteration:\n",
      "[72]\ttrain-merror:0.05515\teval-merror:0.06893\n",
      "\n",
      "[11:12:59] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-merror:0.08108\teval-merror:0.09284\n",
      "Multiple eval metrics have been passed: 'eval-merror' will be used for early stopping.\n",
      "\n",
      "Will train until eval-merror hasn't improved in 20 rounds.\n",
      "[1]\ttrain-merror:0.08049\teval-merror:0.09269\n",
      "[2]\ttrain-merror:0.07798\teval-merror:0.08871\n",
      "[3]\ttrain-merror:0.07665\teval-merror:0.08827\n",
      "[4]\ttrain-merror:0.07636\teval-merror:0.08886\n",
      "[5]\ttrain-merror:0.07646\teval-merror:0.08930\n",
      "[6]\ttrain-merror:0.07656\teval-merror:0.08827\n",
      "[7]\ttrain-merror:0.07597\teval-merror:0.08871\n",
      "[8]\ttrain-merror:0.07562\teval-merror:0.08738\n",
      "[9]\ttrain-merror:0.07533\teval-merror:0.08812\n",
      "[10]\ttrain-merror:0.07336\teval-merror:0.08649\n",
      "[11]\ttrain-merror:0.07277\teval-merror:0.08590\n",
      "[12]\ttrain-merror:0.07237\teval-merror:0.08561\n",
      "[13]\ttrain-merror:0.07173\teval-merror:0.08561\n",
      "[14]\ttrain-merror:0.07159\teval-merror:0.08723\n",
      "[15]\ttrain-merror:0.07144\teval-merror:0.08679\n",
      "[16]\ttrain-merror:0.07110\teval-merror:0.08708\n",
      "[17]\ttrain-merror:0.07075\teval-merror:0.08694\n",
      "[18]\ttrain-merror:0.07041\teval-merror:0.08561\n",
      "[19]\ttrain-merror:0.07006\teval-merror:0.08546\n",
      "[20]\ttrain-merror:0.06982\teval-merror:0.08531\n",
      "[21]\ttrain-merror:0.06927\teval-merror:0.08546\n",
      "[22]\ttrain-merror:0.06903\teval-merror:0.08605\n",
      "[23]\ttrain-merror:0.06888\teval-merror:0.08517\n",
      "[24]\ttrain-merror:0.06785\teval-merror:0.08531\n",
      "[25]\ttrain-merror:0.06780\teval-merror:0.08487\n",
      "[26]\ttrain-merror:0.06745\teval-merror:0.08458\n",
      "[27]\ttrain-merror:0.06726\teval-merror:0.08458\n",
      "[28]\ttrain-merror:0.06677\teval-merror:0.08399\n",
      "[29]\ttrain-merror:0.06681\teval-merror:0.08384\n",
      "[30]\ttrain-merror:0.06627\teval-merror:0.08339\n",
      "[31]\ttrain-merror:0.06534\teval-merror:0.08280\n",
      "[32]\ttrain-merror:0.06504\teval-merror:0.08251\n",
      "[33]\ttrain-merror:0.06524\teval-merror:0.08310\n",
      "[34]\ttrain-merror:0.06480\teval-merror:0.08236\n",
      "[35]\ttrain-merror:0.06480\teval-merror:0.08251\n",
      "[36]\ttrain-merror:0.06465\teval-merror:0.08280\n",
      "[37]\ttrain-merror:0.06386\teval-merror:0.08207\n",
      "[38]\ttrain-merror:0.06352\teval-merror:0.08192\n",
      "[39]\ttrain-merror:0.06224\teval-merror:0.08133\n",
      "[40]\ttrain-merror:0.06155\teval-merror:0.08089\n",
      "[41]\ttrain-merror:0.06135\teval-merror:0.08000\n",
      "[42]\ttrain-merror:0.06091\teval-merror:0.07985\n",
      "[43]\ttrain-merror:0.06066\teval-merror:0.07985\n",
      "[44]\ttrain-merror:0.06042\teval-merror:0.08015\n",
      "[45]\ttrain-merror:0.06032\teval-merror:0.08000\n",
      "[46]\ttrain-merror:0.06012\teval-merror:0.08015\n",
      "[47]\ttrain-merror:0.05963\teval-merror:0.07985\n",
      "[48]\ttrain-merror:0.05983\teval-merror:0.07985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49]\ttrain-merror:0.05919\teval-merror:0.08000\n",
      "[50]\ttrain-merror:0.05909\teval-merror:0.08000\n",
      "[51]\ttrain-merror:0.05870\teval-merror:0.07985\n",
      "[52]\ttrain-merror:0.05835\teval-merror:0.08000\n",
      "[53]\ttrain-merror:0.05806\teval-merror:0.07985\n",
      "[54]\ttrain-merror:0.05781\teval-merror:0.07956\n",
      "[55]\ttrain-merror:0.05766\teval-merror:0.07926\n",
      "[56]\ttrain-merror:0.05697\teval-merror:0.07970\n",
      "[57]\ttrain-merror:0.05648\teval-merror:0.07941\n",
      "[58]\ttrain-merror:0.05624\teval-merror:0.07911\n",
      "[59]\ttrain-merror:0.05599\teval-merror:0.07926\n",
      "[60]\ttrain-merror:0.05530\teval-merror:0.07956\n",
      "[61]\ttrain-merror:0.05496\teval-merror:0.07926\n",
      "[62]\ttrain-merror:0.05456\teval-merror:0.07911\n",
      "[63]\ttrain-merror:0.05422\teval-merror:0.07897\n",
      "[64]\ttrain-merror:0.05378\teval-merror:0.07867\n",
      "[65]\ttrain-merror:0.05378\teval-merror:0.07838\n",
      "[66]\ttrain-merror:0.05343\teval-merror:0.07823\n",
      "[67]\ttrain-merror:0.05333\teval-merror:0.07793\n",
      "[68]\ttrain-merror:0.05304\teval-merror:0.07764\n",
      "[69]\ttrain-merror:0.05269\teval-merror:0.07749\n",
      "[70]\ttrain-merror:0.05260\teval-merror:0.07705\n",
      "[71]\ttrain-merror:0.05220\teval-merror:0.07749\n",
      "[72]\ttrain-merror:0.05186\teval-merror:0.07749\n",
      "[73]\ttrain-merror:0.05176\teval-merror:0.07734\n",
      "[74]\ttrain-merror:0.05132\teval-merror:0.07705\n",
      "[75]\ttrain-merror:0.05107\teval-merror:0.07720\n",
      "[76]\ttrain-merror:0.05078\teval-merror:0.07705\n",
      "[77]\ttrain-merror:0.05053\teval-merror:0.07661\n",
      "[78]\ttrain-merror:0.05033\teval-merror:0.07705\n",
      "[79]\ttrain-merror:0.05009\teval-merror:0.07705\n",
      "[80]\ttrain-merror:0.04974\teval-merror:0.07705\n",
      "[81]\ttrain-merror:0.04945\teval-merror:0.07705\n",
      "[82]\ttrain-merror:0.04920\teval-merror:0.07764\n",
      "[83]\ttrain-merror:0.04895\teval-merror:0.07749\n",
      "[84]\ttrain-merror:0.04871\teval-merror:0.07720\n",
      "[85]\ttrain-merror:0.04841\teval-merror:0.07705\n",
      "[86]\ttrain-merror:0.04812\teval-merror:0.07749\n",
      "[87]\ttrain-merror:0.04772\teval-merror:0.07690\n",
      "[88]\ttrain-merror:0.04738\teval-merror:0.07705\n",
      "[89]\ttrain-merror:0.04684\teval-merror:0.07734\n",
      "[90]\ttrain-merror:0.04659\teval-merror:0.07720\n",
      "[91]\ttrain-merror:0.04630\teval-merror:0.07690\n",
      "[92]\ttrain-merror:0.04615\teval-merror:0.07690\n",
      "[93]\ttrain-merror:0.04605\teval-merror:0.07690\n",
      "[94]\ttrain-merror:0.04590\teval-merror:0.07675\n",
      "[95]\ttrain-merror:0.04566\teval-merror:0.07675\n",
      "[96]\ttrain-merror:0.04546\teval-merror:0.07690\n",
      "[97]\ttrain-merror:0.04512\teval-merror:0.07631\n",
      "[98]\ttrain-merror:0.04521\teval-merror:0.07631\n",
      "[99]\ttrain-merror:0.04482\teval-merror:0.07646\n",
      "[100]\ttrain-merror:0.04448\teval-merror:0.07675\n",
      "[101]\ttrain-merror:0.04428\teval-merror:0.07705\n",
      "[102]\ttrain-merror:0.04403\teval-merror:0.07675\n",
      "[103]\ttrain-merror:0.04384\teval-merror:0.07690\n",
      "[104]\ttrain-merror:0.04364\teval-merror:0.07705\n",
      "[105]\ttrain-merror:0.04354\teval-merror:0.07675\n",
      "[106]\ttrain-merror:0.04325\teval-merror:0.07675\n",
      "[107]\ttrain-merror:0.04295\teval-merror:0.07631\n",
      "[108]\ttrain-merror:0.04276\teval-merror:0.07631\n",
      "[109]\ttrain-merror:0.04246\teval-merror:0.07616\n",
      "[110]\ttrain-merror:0.04192\teval-merror:0.07646\n",
      "[111]\ttrain-merror:0.04187\teval-merror:0.07601\n",
      "[112]\ttrain-merror:0.04157\teval-merror:0.07587\n",
      "[113]\ttrain-merror:0.04152\teval-merror:0.07601\n",
      "[114]\ttrain-merror:0.04157\teval-merror:0.07601\n",
      "[115]\ttrain-merror:0.04118\teval-merror:0.07601\n",
      "[116]\ttrain-merror:0.04093\teval-merror:0.07616\n",
      "[117]\ttrain-merror:0.04074\teval-merror:0.07616\n",
      "[118]\ttrain-merror:0.04044\teval-merror:0.07587\n",
      "[119]\ttrain-merror:0.04025\teval-merror:0.07587\n",
      "[120]\ttrain-merror:0.03985\teval-merror:0.07601\n",
      "[121]\ttrain-merror:0.03970\teval-merror:0.07587\n",
      "[122]\ttrain-merror:0.03956\teval-merror:0.07616\n",
      "[123]\ttrain-merror:0.03936\teval-merror:0.07601\n",
      "[124]\ttrain-merror:0.03936\teval-merror:0.07601\n",
      "[125]\ttrain-merror:0.03911\teval-merror:0.07601\n",
      "[126]\ttrain-merror:0.03897\teval-merror:0.07616\n",
      "[127]\ttrain-merror:0.03877\teval-merror:0.07631\n",
      "[128]\ttrain-merror:0.03857\teval-merror:0.07631\n",
      "[129]\ttrain-merror:0.03843\teval-merror:0.07661\n",
      "[130]\ttrain-merror:0.03833\teval-merror:0.07675\n",
      "[131]\ttrain-merror:0.03828\teval-merror:0.07661\n",
      "[132]\ttrain-merror:0.03803\teval-merror:0.07646\n",
      "Stopping. Best iteration:\n",
      "[112]\ttrain-merror:0.04157\teval-merror:0.07587\n",
      "\n",
      "[11:13:06] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-merror:0.07961\teval-merror:0.08945\n",
      "Multiple eval metrics have been passed: 'eval-merror' will be used for early stopping.\n",
      "\n",
      "Will train until eval-merror hasn't improved in 20 rounds.\n",
      "[1]\ttrain-merror:0.07916\teval-merror:0.08959\n",
      "[2]\ttrain-merror:0.07892\teval-merror:0.08915\n",
      "[3]\ttrain-merror:0.07995\teval-merror:0.09225\n",
      "[4]\ttrain-merror:0.08034\teval-merror:0.09166\n",
      "[5]\ttrain-merror:0.07916\teval-merror:0.08900\n",
      "[6]\ttrain-merror:0.07764\teval-merror:0.08856\n",
      "[7]\ttrain-merror:0.07680\teval-merror:0.08930\n",
      "[8]\ttrain-merror:0.07537\teval-merror:0.08797\n",
      "[9]\ttrain-merror:0.07498\teval-merror:0.08768\n",
      "[10]\ttrain-merror:0.07474\teval-merror:0.08694\n",
      "[11]\ttrain-merror:0.07503\teval-merror:0.08664\n",
      "[12]\ttrain-merror:0.07277\teval-merror:0.08561\n",
      "[13]\ttrain-merror:0.07134\teval-merror:0.08531\n",
      "[14]\ttrain-merror:0.07168\teval-merror:0.08561\n",
      "[15]\ttrain-merror:0.07164\teval-merror:0.08472\n",
      "[16]\ttrain-merror:0.07095\teval-merror:0.08517\n",
      "[17]\ttrain-merror:0.07065\teval-merror:0.08428\n",
      "[18]\ttrain-merror:0.07026\teval-merror:0.08472\n",
      "[19]\ttrain-merror:0.06977\teval-merror:0.08487\n",
      "[20]\ttrain-merror:0.06942\teval-merror:0.08531\n",
      "[21]\ttrain-merror:0.06962\teval-merror:0.08487\n",
      "[22]\ttrain-merror:0.06957\teval-merror:0.08472\n",
      "[23]\ttrain-merror:0.06903\teval-merror:0.08443\n",
      "[24]\ttrain-merror:0.06937\teval-merror:0.08517\n",
      "[25]\ttrain-merror:0.06839\teval-merror:0.08472\n",
      "[26]\ttrain-merror:0.06785\teval-merror:0.08413\n",
      "[27]\ttrain-merror:0.06750\teval-merror:0.08443\n",
      "[28]\ttrain-merror:0.06711\teval-merror:0.08354\n",
      "[29]\ttrain-merror:0.06647\teval-merror:0.08310\n",
      "[30]\ttrain-merror:0.06637\teval-merror:0.08339\n",
      "[31]\ttrain-merror:0.06588\teval-merror:0.08325\n",
      "[32]\ttrain-merror:0.06549\teval-merror:0.08384\n",
      "[33]\ttrain-merror:0.06558\teval-merror:0.08413\n",
      "[34]\ttrain-merror:0.06519\teval-merror:0.08384\n",
      "[35]\ttrain-merror:0.06455\teval-merror:0.08266\n",
      "[36]\ttrain-merror:0.06450\teval-merror:0.08310\n",
      "[37]\ttrain-merror:0.06416\teval-merror:0.08325\n",
      "[38]\ttrain-merror:0.06371\teval-merror:0.08325\n",
      "[39]\ttrain-merror:0.06357\teval-merror:0.08369\n",
      "[40]\ttrain-merror:0.06317\teval-merror:0.08310\n",
      "[41]\ttrain-merror:0.06327\teval-merror:0.08280\n",
      "[42]\ttrain-merror:0.06293\teval-merror:0.08339\n",
      "[43]\ttrain-merror:0.06253\teval-merror:0.08280\n",
      "[44]\ttrain-merror:0.06145\teval-merror:0.08295\n",
      "[45]\ttrain-merror:0.06106\teval-merror:0.08251\n",
      "[46]\ttrain-merror:0.06042\teval-merror:0.08207\n",
      "[47]\ttrain-merror:0.05973\teval-merror:0.08207\n",
      "[48]\ttrain-merror:0.05978\teval-merror:0.08177\n",
      "[49]\ttrain-merror:0.05943\teval-merror:0.08177\n",
      "[50]\ttrain-merror:0.05924\teval-merror:0.08148\n",
      "[51]\ttrain-merror:0.05904\teval-merror:0.08133\n",
      "[52]\ttrain-merror:0.05860\teval-merror:0.08118\n",
      "[53]\ttrain-merror:0.05830\teval-merror:0.08148\n",
      "[54]\ttrain-merror:0.05791\teval-merror:0.08148\n",
      "[55]\ttrain-merror:0.05776\teval-merror:0.08148\n",
      "[56]\ttrain-merror:0.05742\teval-merror:0.08148\n",
      "[57]\ttrain-merror:0.05727\teval-merror:0.08133\n",
      "[58]\ttrain-merror:0.05712\teval-merror:0.08133\n",
      "[59]\ttrain-merror:0.05668\teval-merror:0.08148\n",
      "[60]\ttrain-merror:0.05624\teval-merror:0.08177\n",
      "[61]\ttrain-merror:0.05570\teval-merror:0.08148\n",
      "[62]\ttrain-merror:0.05511\teval-merror:0.08177\n",
      "[63]\ttrain-merror:0.05476\teval-merror:0.08177\n",
      "[64]\ttrain-merror:0.05427\teval-merror:0.08177\n",
      "[65]\ttrain-merror:0.05417\teval-merror:0.08162\n",
      "[66]\ttrain-merror:0.05368\teval-merror:0.08148\n",
      "[67]\ttrain-merror:0.05343\teval-merror:0.08118\n",
      "[68]\ttrain-merror:0.05333\teval-merror:0.08148\n",
      "[69]\ttrain-merror:0.05319\teval-merror:0.08089\n",
      "[70]\ttrain-merror:0.05274\teval-merror:0.08103\n",
      "[71]\ttrain-merror:0.05230\teval-merror:0.08118\n",
      "[72]\ttrain-merror:0.05210\teval-merror:0.08059\n",
      "[73]\ttrain-merror:0.05191\teval-merror:0.08089\n",
      "[74]\ttrain-merror:0.05186\teval-merror:0.08059\n",
      "[75]\ttrain-merror:0.05171\teval-merror:0.08044\n",
      "[76]\ttrain-merror:0.05137\teval-merror:0.08059\n",
      "[77]\ttrain-merror:0.05122\teval-merror:0.08044\n",
      "[78]\ttrain-merror:0.05102\teval-merror:0.08030\n",
      "[79]\ttrain-merror:0.05102\teval-merror:0.08030\n",
      "[80]\ttrain-merror:0.05097\teval-merror:0.08030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[81]\ttrain-merror:0.05073\teval-merror:0.08030\n",
      "[82]\ttrain-merror:0.05048\teval-merror:0.08030\n",
      "[83]\ttrain-merror:0.05009\teval-merror:0.08000\n",
      "[84]\ttrain-merror:0.04964\teval-merror:0.08000\n",
      "[85]\ttrain-merror:0.04930\teval-merror:0.08015\n",
      "[86]\ttrain-merror:0.04900\teval-merror:0.08044\n",
      "[87]\ttrain-merror:0.04895\teval-merror:0.08030\n",
      "[88]\ttrain-merror:0.04846\teval-merror:0.08015\n",
      "[89]\ttrain-merror:0.04812\teval-merror:0.08015\n",
      "[90]\ttrain-merror:0.04797\teval-merror:0.08000\n",
      "[91]\ttrain-merror:0.04758\teval-merror:0.07911\n",
      "[92]\ttrain-merror:0.04738\teval-merror:0.07941\n",
      "[93]\ttrain-merror:0.04679\teval-merror:0.07941\n",
      "[94]\ttrain-merror:0.04679\teval-merror:0.07956\n",
      "[95]\ttrain-merror:0.04640\teval-merror:0.07926\n",
      "[96]\ttrain-merror:0.04556\teval-merror:0.07956\n",
      "[97]\ttrain-merror:0.04536\teval-merror:0.07956\n",
      "[98]\ttrain-merror:0.04531\teval-merror:0.07941\n",
      "[99]\ttrain-merror:0.04487\teval-merror:0.07970\n",
      "[100]\ttrain-merror:0.04467\teval-merror:0.07941\n",
      "[101]\ttrain-merror:0.04453\teval-merror:0.07941\n",
      "[102]\ttrain-merror:0.04399\teval-merror:0.07956\n",
      "[103]\ttrain-merror:0.04384\teval-merror:0.07911\n",
      "[104]\ttrain-merror:0.04374\teval-merror:0.07926\n",
      "[105]\ttrain-merror:0.04364\teval-merror:0.07926\n",
      "[106]\ttrain-merror:0.04369\teval-merror:0.07941\n",
      "[107]\ttrain-merror:0.04335\teval-merror:0.07926\n",
      "[108]\ttrain-merror:0.04305\teval-merror:0.07897\n",
      "[109]\ttrain-merror:0.04310\teval-merror:0.07882\n",
      "[110]\ttrain-merror:0.04280\teval-merror:0.07911\n",
      "[111]\ttrain-merror:0.04266\teval-merror:0.07882\n",
      "[112]\ttrain-merror:0.04231\teval-merror:0.07882\n",
      "[113]\ttrain-merror:0.04207\teval-merror:0.07838\n",
      "[114]\ttrain-merror:0.04182\teval-merror:0.07867\n",
      "[115]\ttrain-merror:0.04167\teval-merror:0.07838\n",
      "[116]\ttrain-merror:0.04157\teval-merror:0.07882\n",
      "[117]\ttrain-merror:0.04123\teval-merror:0.07897\n",
      "[118]\ttrain-merror:0.04118\teval-merror:0.07911\n",
      "[119]\ttrain-merror:0.04108\teval-merror:0.07926\n",
      "[120]\ttrain-merror:0.04064\teval-merror:0.07911\n",
      "[121]\ttrain-merror:0.04059\teval-merror:0.07911\n",
      "[122]\ttrain-merror:0.04049\teval-merror:0.07926\n",
      "[123]\ttrain-merror:0.04015\teval-merror:0.07897\n",
      "[124]\ttrain-merror:0.03946\teval-merror:0.07897\n",
      "[125]\ttrain-merror:0.03926\teval-merror:0.07882\n",
      "[126]\ttrain-merror:0.03911\teval-merror:0.07897\n",
      "[127]\ttrain-merror:0.03872\teval-merror:0.07882\n",
      "[128]\ttrain-merror:0.03848\teval-merror:0.07911\n",
      "[129]\ttrain-merror:0.03848\teval-merror:0.07926\n",
      "[130]\ttrain-merror:0.03828\teval-merror:0.07941\n",
      "[131]\ttrain-merror:0.03803\teval-merror:0.07941\n",
      "[132]\ttrain-merror:0.03779\teval-merror:0.07941\n",
      "[133]\ttrain-merror:0.03720\teval-merror:0.07926\n",
      "Stopping. Best iteration:\n",
      "[113]\ttrain-merror:0.04207\teval-merror:0.07838\n",
      "\n",
      "[11:13:14] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-merror:0.09230\teval-merror:0.09653\n",
      "Multiple eval metrics have been passed: 'eval-merror' will be used for early stopping.\n",
      "\n",
      "Will train until eval-merror hasn't improved in 20 rounds.\n",
      "[1]\ttrain-merror:0.09314\teval-merror:0.09506\n",
      "[2]\ttrain-merror:0.09210\teval-merror:0.09506\n",
      "[3]\ttrain-merror:0.09186\teval-merror:0.09476\n",
      "[4]\ttrain-merror:0.09127\teval-merror:0.09358\n",
      "[5]\ttrain-merror:0.09137\teval-merror:0.09388\n",
      "[6]\ttrain-merror:0.09141\teval-merror:0.09417\n",
      "[7]\ttrain-merror:0.09038\teval-merror:0.09476\n",
      "[8]\ttrain-merror:0.09058\teval-merror:0.09476\n",
      "[9]\ttrain-merror:0.08989\teval-merror:0.09432\n",
      "[10]\ttrain-merror:0.08950\teval-merror:0.09343\n",
      "[11]\ttrain-merror:0.08969\teval-merror:0.09373\n",
      "[12]\ttrain-merror:0.08994\teval-merror:0.09358\n",
      "[13]\ttrain-merror:0.08989\teval-merror:0.09328\n",
      "[14]\ttrain-merror:0.09004\teval-merror:0.09269\n",
      "[15]\ttrain-merror:0.08950\teval-merror:0.09299\n",
      "[16]\ttrain-merror:0.08959\teval-merror:0.09328\n",
      "[17]\ttrain-merror:0.08994\teval-merror:0.09328\n",
      "[18]\ttrain-merror:0.08964\teval-merror:0.09328\n",
      "[19]\ttrain-merror:0.08920\teval-merror:0.09284\n",
      "[20]\ttrain-merror:0.08900\teval-merror:0.09299\n",
      "[21]\ttrain-merror:0.08881\teval-merror:0.09284\n",
      "[22]\ttrain-merror:0.08886\teval-merror:0.09328\n",
      "[23]\ttrain-merror:0.08925\teval-merror:0.09328\n",
      "[24]\ttrain-merror:0.08930\teval-merror:0.09314\n",
      "[25]\ttrain-merror:0.08950\teval-merror:0.09328\n",
      "[26]\ttrain-merror:0.08876\teval-merror:0.09196\n",
      "[27]\ttrain-merror:0.08895\teval-merror:0.09181\n",
      "[28]\ttrain-merror:0.08768\teval-merror:0.09077\n",
      "[29]\ttrain-merror:0.08654\teval-merror:0.08886\n",
      "[30]\ttrain-merror:0.08659\teval-merror:0.08945\n",
      "[31]\ttrain-merror:0.08625\teval-merror:0.08856\n",
      "[32]\ttrain-merror:0.08620\teval-merror:0.08856\n",
      "[33]\ttrain-merror:0.08610\teval-merror:0.08871\n",
      "[34]\ttrain-merror:0.08561\teval-merror:0.08827\n",
      "[35]\ttrain-merror:0.08502\teval-merror:0.08694\n",
      "[36]\ttrain-merror:0.08418\teval-merror:0.08635\n",
      "[37]\ttrain-merror:0.08290\teval-merror:0.08546\n",
      "[38]\ttrain-merror:0.08197\teval-merror:0.08487\n",
      "[39]\ttrain-merror:0.08167\teval-merror:0.08413\n",
      "[40]\ttrain-merror:0.08094\teval-merror:0.08369\n",
      "[41]\ttrain-merror:0.08069\teval-merror:0.08295\n",
      "[42]\ttrain-merror:0.08015\teval-merror:0.08266\n",
      "[43]\ttrain-merror:0.07985\teval-merror:0.08207\n",
      "[44]\ttrain-merror:0.07916\teval-merror:0.08148\n",
      "[45]\ttrain-merror:0.07892\teval-merror:0.08133\n",
      "[46]\ttrain-merror:0.07848\teval-merror:0.08133\n",
      "[47]\ttrain-merror:0.07808\teval-merror:0.08133\n",
      "[48]\ttrain-merror:0.07779\teval-merror:0.08133\n",
      "[49]\ttrain-merror:0.07744\teval-merror:0.08148\n",
      "[50]\ttrain-merror:0.07729\teval-merror:0.08133\n",
      "[51]\ttrain-merror:0.07675\teval-merror:0.08044\n",
      "[52]\ttrain-merror:0.07621\teval-merror:0.08118\n",
      "[53]\ttrain-merror:0.07636\teval-merror:0.08177\n",
      "[54]\ttrain-merror:0.07567\teval-merror:0.08177\n",
      "[55]\ttrain-merror:0.07523\teval-merror:0.08148\n",
      "[56]\ttrain-merror:0.07488\teval-merror:0.08162\n",
      "[57]\ttrain-merror:0.07444\teval-merror:0.08103\n",
      "[58]\ttrain-merror:0.07464\teval-merror:0.08177\n",
      "[59]\ttrain-merror:0.07385\teval-merror:0.08162\n",
      "[60]\ttrain-merror:0.07390\teval-merror:0.08148\n",
      "[61]\ttrain-merror:0.07365\teval-merror:0.08133\n",
      "[62]\ttrain-merror:0.07326\teval-merror:0.08074\n",
      "[63]\ttrain-merror:0.07291\teval-merror:0.08044\n",
      "[64]\ttrain-merror:0.07282\teval-merror:0.08059\n",
      "[65]\ttrain-merror:0.07252\teval-merror:0.08015\n",
      "[66]\ttrain-merror:0.07252\teval-merror:0.08015\n",
      "[67]\ttrain-merror:0.07203\teval-merror:0.07985\n",
      "[68]\ttrain-merror:0.07173\teval-merror:0.07970\n",
      "[69]\ttrain-merror:0.07168\teval-merror:0.07926\n",
      "[70]\ttrain-merror:0.07159\teval-merror:0.08000\n",
      "[71]\ttrain-merror:0.07129\teval-merror:0.08044\n",
      "[72]\ttrain-merror:0.07090\teval-merror:0.07985\n",
      "[73]\ttrain-merror:0.07050\teval-merror:0.08015\n",
      "[74]\ttrain-merror:0.06982\teval-merror:0.07985\n",
      "[75]\ttrain-merror:0.06996\teval-merror:0.07970\n",
      "[76]\ttrain-merror:0.06947\teval-merror:0.07985\n",
      "[77]\ttrain-merror:0.06947\teval-merror:0.08000\n",
      "[78]\ttrain-merror:0.06972\teval-merror:0.07985\n",
      "[79]\ttrain-merror:0.06957\teval-merror:0.07970\n",
      "[80]\ttrain-merror:0.06932\teval-merror:0.07970\n",
      "[81]\ttrain-merror:0.06918\teval-merror:0.07956\n",
      "[82]\ttrain-merror:0.06883\teval-merror:0.07897\n",
      "[83]\ttrain-merror:0.06873\teval-merror:0.07911\n",
      "[84]\ttrain-merror:0.06868\teval-merror:0.07852\n",
      "[85]\ttrain-merror:0.06844\teval-merror:0.07823\n",
      "[86]\ttrain-merror:0.06858\teval-merror:0.07867\n",
      "[87]\ttrain-merror:0.06844\teval-merror:0.07882\n",
      "[88]\ttrain-merror:0.06819\teval-merror:0.07852\n",
      "[89]\ttrain-merror:0.06785\teval-merror:0.07838\n",
      "[90]\ttrain-merror:0.06770\teval-merror:0.07838\n",
      "[91]\ttrain-merror:0.06760\teval-merror:0.07823\n",
      "[92]\ttrain-merror:0.06760\teval-merror:0.07793\n",
      "[93]\ttrain-merror:0.06731\teval-merror:0.07793\n",
      "[94]\ttrain-merror:0.06691\teval-merror:0.07808\n",
      "[95]\ttrain-merror:0.06686\teval-merror:0.07823\n",
      "[96]\ttrain-merror:0.06681\teval-merror:0.07867\n",
      "[97]\ttrain-merror:0.06657\teval-merror:0.07897\n",
      "[98]\ttrain-merror:0.06632\teval-merror:0.07897\n",
      "[99]\ttrain-merror:0.06637\teval-merror:0.07882\n",
      "[100]\ttrain-merror:0.06637\teval-merror:0.07882\n",
      "[101]\ttrain-merror:0.06593\teval-merror:0.07882\n",
      "[102]\ttrain-merror:0.06573\teval-merror:0.07882\n",
      "[103]\ttrain-merror:0.06553\teval-merror:0.07882\n",
      "[104]\ttrain-merror:0.06529\teval-merror:0.07882\n",
      "[105]\ttrain-merror:0.06509\teval-merror:0.07882\n",
      "[106]\ttrain-merror:0.06499\teval-merror:0.07911\n",
      "[107]\ttrain-merror:0.06460\teval-merror:0.07867\n",
      "[108]\ttrain-merror:0.06445\teval-merror:0.07852\n",
      "[109]\ttrain-merror:0.06445\teval-merror:0.07867\n",
      "[110]\ttrain-merror:0.06440\teval-merror:0.07867\n",
      "[111]\ttrain-merror:0.06376\teval-merror:0.07852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[112]\ttrain-merror:0.06362\teval-merror:0.07852\n",
      "Stopping. Best iteration:\n",
      "[92]\ttrain-merror:0.06760\teval-merror:0.07793\n",
      "\n",
      "[11:13:19] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-merror:0.09392\teval-merror:0.09077\n",
      "Multiple eval metrics have been passed: 'eval-merror' will be used for early stopping.\n",
      "\n",
      "Will train until eval-merror hasn't improved in 20 rounds.\n",
      "[1]\ttrain-merror:0.09476\teval-merror:0.09107\n",
      "[2]\ttrain-merror:0.09368\teval-merror:0.09063\n",
      "[3]\ttrain-merror:0.09309\teval-merror:0.09063\n",
      "[4]\ttrain-merror:0.09333\teval-merror:0.08989\n",
      "[5]\ttrain-merror:0.09314\teval-merror:0.09048\n",
      "[6]\ttrain-merror:0.09289\teval-merror:0.09004\n",
      "[7]\ttrain-merror:0.09255\teval-merror:0.09004\n",
      "[8]\ttrain-merror:0.09279\teval-merror:0.08989\n",
      "[9]\ttrain-merror:0.09265\teval-merror:0.09019\n",
      "[10]\ttrain-merror:0.09215\teval-merror:0.09033\n",
      "[11]\ttrain-merror:0.09230\teval-merror:0.08989\n",
      "[12]\ttrain-merror:0.09220\teval-merror:0.09019\n",
      "[13]\ttrain-merror:0.09171\teval-merror:0.09004\n",
      "[14]\ttrain-merror:0.09176\teval-merror:0.09004\n",
      "[15]\ttrain-merror:0.09141\teval-merror:0.08989\n",
      "[16]\ttrain-merror:0.09141\teval-merror:0.09033\n",
      "[17]\ttrain-merror:0.09117\teval-merror:0.08959\n",
      "[18]\ttrain-merror:0.09166\teval-merror:0.09033\n",
      "[19]\ttrain-merror:0.09156\teval-merror:0.09033\n",
      "[20]\ttrain-merror:0.09146\teval-merror:0.09004\n",
      "[21]\ttrain-merror:0.09132\teval-merror:0.09004\n",
      "[22]\ttrain-merror:0.09176\teval-merror:0.09033\n",
      "[23]\ttrain-merror:0.09171\teval-merror:0.09004\n",
      "[24]\ttrain-merror:0.08979\teval-merror:0.08753\n",
      "[25]\ttrain-merror:0.08935\teval-merror:0.08664\n",
      "[26]\ttrain-merror:0.08945\teval-merror:0.08649\n",
      "[27]\ttrain-merror:0.08758\teval-merror:0.08502\n",
      "[28]\ttrain-merror:0.08743\teval-merror:0.08487\n",
      "[29]\ttrain-merror:0.08728\teval-merror:0.08443\n",
      "[30]\ttrain-merror:0.08708\teval-merror:0.08458\n",
      "[31]\ttrain-merror:0.08630\teval-merror:0.08428\n",
      "[32]\ttrain-merror:0.08571\teval-merror:0.08399\n",
      "[33]\ttrain-merror:0.08556\teval-merror:0.08384\n",
      "[34]\ttrain-merror:0.08521\teval-merror:0.08369\n",
      "[35]\ttrain-merror:0.08458\teval-merror:0.08236\n",
      "[36]\ttrain-merror:0.08472\teval-merror:0.08177\n",
      "[37]\ttrain-merror:0.08413\teval-merror:0.08148\n",
      "[38]\ttrain-merror:0.08364\teval-merror:0.08133\n",
      "[39]\ttrain-merror:0.08374\teval-merror:0.08103\n",
      "[40]\ttrain-merror:0.08379\teval-merror:0.08133\n",
      "[41]\ttrain-merror:0.08354\teval-merror:0.08162\n",
      "[42]\ttrain-merror:0.08344\teval-merror:0.08162\n",
      "[43]\ttrain-merror:0.08305\teval-merror:0.08059\n",
      "[44]\ttrain-merror:0.08236\teval-merror:0.07882\n",
      "[45]\ttrain-merror:0.08207\teval-merror:0.07956\n",
      "[46]\ttrain-merror:0.08192\teval-merror:0.07941\n",
      "[47]\ttrain-merror:0.08177\teval-merror:0.07882\n",
      "[48]\ttrain-merror:0.08153\teval-merror:0.07882\n",
      "[49]\ttrain-merror:0.08123\teval-merror:0.07882\n",
      "[50]\ttrain-merror:0.08113\teval-merror:0.07882\n",
      "[51]\ttrain-merror:0.08098\teval-merror:0.07793\n",
      "[52]\ttrain-merror:0.08049\teval-merror:0.07779\n",
      "[53]\ttrain-merror:0.07995\teval-merror:0.07734\n",
      "[54]\ttrain-merror:0.07966\teval-merror:0.07720\n",
      "[55]\ttrain-merror:0.07921\teval-merror:0.07720\n",
      "[56]\ttrain-merror:0.07902\teval-merror:0.07720\n",
      "[57]\ttrain-merror:0.07887\teval-merror:0.07675\n",
      "[58]\ttrain-merror:0.07828\teval-merror:0.07675\n",
      "[59]\ttrain-merror:0.07798\teval-merror:0.07601\n",
      "[60]\ttrain-merror:0.07774\teval-merror:0.07631\n",
      "[61]\ttrain-merror:0.07739\teval-merror:0.07616\n",
      "[62]\ttrain-merror:0.07715\teval-merror:0.07572\n",
      "[63]\ttrain-merror:0.07661\teval-merror:0.07587\n",
      "[64]\ttrain-merror:0.07641\teval-merror:0.07498\n",
      "[65]\ttrain-merror:0.07621\teval-merror:0.07498\n",
      "[66]\ttrain-merror:0.07616\teval-merror:0.07424\n",
      "[67]\ttrain-merror:0.07597\teval-merror:0.07395\n",
      "[68]\ttrain-merror:0.07577\teval-merror:0.07380\n",
      "[69]\ttrain-merror:0.07587\teval-merror:0.07336\n",
      "[70]\ttrain-merror:0.07577\teval-merror:0.07351\n",
      "[71]\ttrain-merror:0.07557\teval-merror:0.07351\n",
      "[72]\ttrain-merror:0.07503\teval-merror:0.07351\n",
      "[73]\ttrain-merror:0.07479\teval-merror:0.07291\n",
      "[74]\ttrain-merror:0.07464\teval-merror:0.07321\n",
      "[75]\ttrain-merror:0.07459\teval-merror:0.07277\n",
      "[76]\ttrain-merror:0.07439\teval-merror:0.07277\n",
      "[77]\ttrain-merror:0.07444\teval-merror:0.07306\n",
      "[78]\ttrain-merror:0.07439\teval-merror:0.07291\n",
      "[79]\ttrain-merror:0.07415\teval-merror:0.07291\n",
      "[80]\ttrain-merror:0.07405\teval-merror:0.07291\n",
      "[81]\ttrain-merror:0.07365\teval-merror:0.07277\n",
      "[82]\ttrain-merror:0.07375\teval-merror:0.07277\n",
      "[83]\ttrain-merror:0.07351\teval-merror:0.07262\n",
      "[84]\ttrain-merror:0.07341\teval-merror:0.07306\n",
      "[85]\ttrain-merror:0.07321\teval-merror:0.07306\n",
      "[86]\ttrain-merror:0.07306\teval-merror:0.07306\n",
      "[87]\ttrain-merror:0.07257\teval-merror:0.07291\n",
      "[88]\ttrain-merror:0.07223\teval-merror:0.07291\n",
      "[89]\ttrain-merror:0.07203\teval-merror:0.07306\n",
      "[90]\ttrain-merror:0.07183\teval-merror:0.07306\n",
      "[91]\ttrain-merror:0.07159\teval-merror:0.07291\n",
      "[92]\ttrain-merror:0.07114\teval-merror:0.07262\n",
      "[93]\ttrain-merror:0.07105\teval-merror:0.07262\n",
      "[94]\ttrain-merror:0.07114\teval-merror:0.07291\n",
      "[95]\ttrain-merror:0.07110\teval-merror:0.07291\n",
      "[96]\ttrain-merror:0.07075\teval-merror:0.07306\n",
      "[97]\ttrain-merror:0.07070\teval-merror:0.07262\n",
      "[98]\ttrain-merror:0.07060\teval-merror:0.07247\n",
      "[99]\ttrain-merror:0.07036\teval-merror:0.07203\n",
      "[100]\ttrain-merror:0.07016\teval-merror:0.07218\n",
      "[101]\ttrain-merror:0.07036\teval-merror:0.07218\n",
      "[102]\ttrain-merror:0.07021\teval-merror:0.07188\n",
      "[103]\ttrain-merror:0.06986\teval-merror:0.07159\n",
      "[104]\ttrain-merror:0.06977\teval-merror:0.07159\n",
      "[105]\ttrain-merror:0.06922\teval-merror:0.07144\n",
      "[106]\ttrain-merror:0.06908\teval-merror:0.07144\n",
      "[107]\ttrain-merror:0.06834\teval-merror:0.07144\n",
      "[108]\ttrain-merror:0.06819\teval-merror:0.07159\n",
      "[109]\ttrain-merror:0.06824\teval-merror:0.07144\n",
      "[110]\ttrain-merror:0.06799\teval-merror:0.07114\n",
      "[111]\ttrain-merror:0.06799\teval-merror:0.07159\n",
      "[112]\ttrain-merror:0.06790\teval-merror:0.07129\n",
      "[113]\ttrain-merror:0.06755\teval-merror:0.07144\n",
      "[114]\ttrain-merror:0.06726\teval-merror:0.07129\n",
      "[115]\ttrain-merror:0.06696\teval-merror:0.07114\n",
      "[116]\ttrain-merror:0.06686\teval-merror:0.07129\n",
      "[117]\ttrain-merror:0.06647\teval-merror:0.07129\n",
      "[118]\ttrain-merror:0.06642\teval-merror:0.07100\n",
      "[119]\ttrain-merror:0.06613\teval-merror:0.07070\n",
      "[120]\ttrain-merror:0.06603\teval-merror:0.07041\n",
      "[121]\ttrain-merror:0.06588\teval-merror:0.07026\n",
      "[122]\ttrain-merror:0.06563\teval-merror:0.07026\n",
      "[123]\ttrain-merror:0.06558\teval-merror:0.07011\n",
      "[124]\ttrain-merror:0.06549\teval-merror:0.06996\n",
      "[125]\ttrain-merror:0.06529\teval-merror:0.07011\n",
      "[126]\ttrain-merror:0.06529\teval-merror:0.07055\n",
      "[127]\ttrain-merror:0.06529\teval-merror:0.07055\n",
      "[128]\ttrain-merror:0.06509\teval-merror:0.07085\n",
      "[129]\ttrain-merror:0.06499\teval-merror:0.07100\n",
      "[130]\ttrain-merror:0.06470\teval-merror:0.07085\n",
      "[131]\ttrain-merror:0.06450\teval-merror:0.07114\n",
      "[132]\ttrain-merror:0.06421\teval-merror:0.07070\n",
      "[133]\ttrain-merror:0.06416\teval-merror:0.07085\n",
      "[134]\ttrain-merror:0.06391\teval-merror:0.07070\n",
      "[135]\ttrain-merror:0.06352\teval-merror:0.07070\n",
      "[136]\ttrain-merror:0.06342\teval-merror:0.07070\n",
      "[137]\ttrain-merror:0.06322\teval-merror:0.07100\n",
      "[138]\ttrain-merror:0.06327\teval-merror:0.07100\n",
      "[139]\ttrain-merror:0.06298\teval-merror:0.07070\n",
      "[140]\ttrain-merror:0.06298\teval-merror:0.07070\n",
      "[141]\ttrain-merror:0.06278\teval-merror:0.07055\n",
      "[142]\ttrain-merror:0.06283\teval-merror:0.07041\n",
      "[143]\ttrain-merror:0.06268\teval-merror:0.07055\n",
      "[144]\ttrain-merror:0.06243\teval-merror:0.07070\n",
      "Stopping. Best iteration:\n",
      "[124]\ttrain-merror:0.06549\teval-merror:0.06996\n",
      "\n",
      "[11:13:25] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-merror:0.09132\teval-merror:0.09565\n",
      "Multiple eval metrics have been passed: 'eval-merror' will be used for early stopping.\n",
      "\n",
      "Will train until eval-merror hasn't improved in 20 rounds.\n",
      "[1]\ttrain-merror:0.09201\teval-merror:0.09565\n",
      "[2]\ttrain-merror:0.09176\teval-merror:0.09697\n",
      "[3]\ttrain-merror:0.09137\teval-merror:0.09653\n",
      "[4]\ttrain-merror:0.09156\teval-merror:0.09579\n",
      "[5]\ttrain-merror:0.09053\teval-merror:0.09506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6]\ttrain-merror:0.08950\teval-merror:0.09491\n",
      "[7]\ttrain-merror:0.08984\teval-merror:0.09506\n",
      "[8]\ttrain-merror:0.08989\teval-merror:0.09491\n",
      "[9]\ttrain-merror:0.08969\teval-merror:0.09520\n",
      "[10]\ttrain-merror:0.08964\teval-merror:0.09506\n",
      "[11]\ttrain-merror:0.08979\teval-merror:0.09461\n",
      "[12]\ttrain-merror:0.08950\teval-merror:0.09476\n",
      "[13]\ttrain-merror:0.08954\teval-merror:0.09446\n",
      "[14]\ttrain-merror:0.08950\teval-merror:0.09476\n",
      "[15]\ttrain-merror:0.08994\teval-merror:0.09446\n",
      "[16]\ttrain-merror:0.08969\teval-merror:0.09520\n",
      "[17]\ttrain-merror:0.08979\teval-merror:0.09506\n",
      "[18]\ttrain-merror:0.08915\teval-merror:0.09594\n",
      "[19]\ttrain-merror:0.08950\teval-merror:0.09579\n",
      "[20]\ttrain-merror:0.08959\teval-merror:0.09550\n",
      "[21]\ttrain-merror:0.08930\teval-merror:0.09550\n",
      "[22]\ttrain-merror:0.08856\teval-merror:0.09565\n",
      "[23]\ttrain-merror:0.08866\teval-merror:0.09565\n",
      "[24]\ttrain-merror:0.08738\teval-merror:0.09446\n",
      "[25]\ttrain-merror:0.08699\teval-merror:0.09461\n",
      "[26]\ttrain-merror:0.08507\teval-merror:0.09284\n",
      "[27]\ttrain-merror:0.08487\teval-merror:0.09314\n",
      "[28]\ttrain-merror:0.08472\teval-merror:0.09328\n",
      "[29]\ttrain-merror:0.08413\teval-merror:0.09255\n",
      "[30]\ttrain-merror:0.08359\teval-merror:0.09240\n",
      "[31]\ttrain-merror:0.08344\teval-merror:0.09137\n",
      "[32]\ttrain-merror:0.08305\teval-merror:0.09122\n",
      "[33]\ttrain-merror:0.08275\teval-merror:0.09137\n",
      "[34]\ttrain-merror:0.08256\teval-merror:0.09122\n",
      "[35]\ttrain-merror:0.08197\teval-merror:0.09077\n",
      "[36]\ttrain-merror:0.08094\teval-merror:0.09048\n",
      "[37]\ttrain-merror:0.08015\teval-merror:0.09033\n",
      "[38]\ttrain-merror:0.07946\teval-merror:0.09004\n",
      "[39]\ttrain-merror:0.07931\teval-merror:0.08974\n",
      "[40]\ttrain-merror:0.07911\teval-merror:0.08945\n",
      "[41]\ttrain-merror:0.07882\teval-merror:0.08945\n",
      "[42]\ttrain-merror:0.07882\teval-merror:0.08945\n",
      "[43]\ttrain-merror:0.07838\teval-merror:0.08900\n",
      "[44]\ttrain-merror:0.07808\teval-merror:0.08915\n",
      "[45]\ttrain-merror:0.07798\teval-merror:0.08915\n",
      "[46]\ttrain-merror:0.07764\teval-merror:0.08900\n",
      "[47]\ttrain-merror:0.07729\teval-merror:0.08871\n",
      "[48]\ttrain-merror:0.07690\teval-merror:0.08856\n",
      "[49]\ttrain-merror:0.07670\teval-merror:0.08812\n",
      "[50]\ttrain-merror:0.07626\teval-merror:0.08768\n",
      "[51]\ttrain-merror:0.07616\teval-merror:0.08723\n",
      "[52]\ttrain-merror:0.07542\teval-merror:0.08694\n",
      "[53]\ttrain-merror:0.07493\teval-merror:0.08708\n",
      "[54]\ttrain-merror:0.07434\teval-merror:0.08723\n",
      "[55]\ttrain-merror:0.07395\teval-merror:0.08546\n",
      "[56]\ttrain-merror:0.07346\teval-merror:0.08517\n",
      "[57]\ttrain-merror:0.07326\teval-merror:0.08487\n",
      "[58]\ttrain-merror:0.07311\teval-merror:0.08487\n",
      "[59]\ttrain-merror:0.07306\teval-merror:0.08458\n",
      "[60]\ttrain-merror:0.07326\teval-merror:0.08458\n",
      "[61]\ttrain-merror:0.07291\teval-merror:0.08413\n",
      "[62]\ttrain-merror:0.07252\teval-merror:0.08413\n",
      "[63]\ttrain-merror:0.07223\teval-merror:0.08399\n",
      "[64]\ttrain-merror:0.07213\teval-merror:0.08384\n",
      "[65]\ttrain-merror:0.07183\teval-merror:0.08354\n",
      "[66]\ttrain-merror:0.07154\teval-merror:0.08354\n",
      "[67]\ttrain-merror:0.07105\teval-merror:0.08384\n",
      "[68]\ttrain-merror:0.07114\teval-merror:0.08413\n",
      "[69]\ttrain-merror:0.07105\teval-merror:0.08443\n",
      "[70]\ttrain-merror:0.07095\teval-merror:0.08443\n",
      "[71]\ttrain-merror:0.07070\teval-merror:0.08413\n",
      "[72]\ttrain-merror:0.07046\teval-merror:0.08384\n",
      "[73]\ttrain-merror:0.07041\teval-merror:0.08339\n",
      "[74]\ttrain-merror:0.07011\teval-merror:0.08310\n",
      "[75]\ttrain-merror:0.06982\teval-merror:0.08339\n",
      "[76]\ttrain-merror:0.06972\teval-merror:0.08295\n",
      "[77]\ttrain-merror:0.06972\teval-merror:0.08310\n",
      "[78]\ttrain-merror:0.06962\teval-merror:0.08325\n",
      "[79]\ttrain-merror:0.06962\teval-merror:0.08310\n",
      "[80]\ttrain-merror:0.06952\teval-merror:0.08310\n",
      "[81]\ttrain-merror:0.06927\teval-merror:0.08280\n",
      "[82]\ttrain-merror:0.06908\teval-merror:0.08236\n",
      "[83]\ttrain-merror:0.06903\teval-merror:0.08207\n",
      "[84]\ttrain-merror:0.06878\teval-merror:0.08221\n",
      "[85]\ttrain-merror:0.06883\teval-merror:0.08221\n",
      "[86]\ttrain-merror:0.06854\teval-merror:0.08207\n",
      "[87]\ttrain-merror:0.06839\teval-merror:0.08192\n",
      "[88]\ttrain-merror:0.06814\teval-merror:0.08192\n",
      "[89]\ttrain-merror:0.06824\teval-merror:0.08177\n",
      "[90]\ttrain-merror:0.06775\teval-merror:0.08148\n",
      "[91]\ttrain-merror:0.06770\teval-merror:0.08103\n",
      "[92]\ttrain-merror:0.06799\teval-merror:0.08030\n",
      "[93]\ttrain-merror:0.06735\teval-merror:0.08044\n",
      "[94]\ttrain-merror:0.06731\teval-merror:0.08103\n",
      "[95]\ttrain-merror:0.06765\teval-merror:0.08103\n",
      "[96]\ttrain-merror:0.06726\teval-merror:0.08074\n",
      "[97]\ttrain-merror:0.06711\teval-merror:0.08044\n",
      "[98]\ttrain-merror:0.06691\teval-merror:0.08059\n",
      "[99]\ttrain-merror:0.06706\teval-merror:0.08030\n",
      "[100]\ttrain-merror:0.06711\teval-merror:0.08030\n",
      "[101]\ttrain-merror:0.06657\teval-merror:0.07970\n",
      "[102]\ttrain-merror:0.06657\teval-merror:0.07985\n",
      "[103]\ttrain-merror:0.06657\teval-merror:0.07970\n",
      "[104]\ttrain-merror:0.06627\teval-merror:0.07985\n",
      "[105]\ttrain-merror:0.06632\teval-merror:0.08015\n",
      "[106]\ttrain-merror:0.06583\teval-merror:0.07985\n",
      "[107]\ttrain-merror:0.06563\teval-merror:0.08000\n",
      "[108]\ttrain-merror:0.06553\teval-merror:0.07985\n",
      "[109]\ttrain-merror:0.06534\teval-merror:0.08015\n",
      "[110]\ttrain-merror:0.06519\teval-merror:0.08044\n",
      "[111]\ttrain-merror:0.06480\teval-merror:0.07985\n",
      "[112]\ttrain-merror:0.06470\teval-merror:0.07926\n",
      "[113]\ttrain-merror:0.06465\teval-merror:0.07882\n",
      "[114]\ttrain-merror:0.06465\teval-merror:0.07941\n",
      "[115]\ttrain-merror:0.06460\teval-merror:0.07882\n",
      "[116]\ttrain-merror:0.06426\teval-merror:0.07823\n",
      "[117]\ttrain-merror:0.06406\teval-merror:0.07852\n",
      "[118]\ttrain-merror:0.06401\teval-merror:0.07823\n",
      "[119]\ttrain-merror:0.06366\teval-merror:0.07852\n",
      "[120]\ttrain-merror:0.06357\teval-merror:0.07852\n",
      "[121]\ttrain-merror:0.06376\teval-merror:0.07867\n",
      "[122]\ttrain-merror:0.06352\teval-merror:0.07897\n",
      "[123]\ttrain-merror:0.06337\teval-merror:0.07838\n",
      "[124]\ttrain-merror:0.06337\teval-merror:0.07823\n",
      "[125]\ttrain-merror:0.06327\teval-merror:0.07838\n",
      "[126]\ttrain-merror:0.06293\teval-merror:0.07852\n",
      "[127]\ttrain-merror:0.06298\teval-merror:0.07852\n",
      "[128]\ttrain-merror:0.06273\teval-merror:0.07823\n",
      "[129]\ttrain-merror:0.06268\teval-merror:0.07779\n",
      "[130]\ttrain-merror:0.06243\teval-merror:0.07808\n",
      "[131]\ttrain-merror:0.06224\teval-merror:0.07852\n",
      "[132]\ttrain-merror:0.06184\teval-merror:0.07838\n",
      "[133]\ttrain-merror:0.06184\teval-merror:0.07823\n",
      "[134]\ttrain-merror:0.06155\teval-merror:0.07838\n",
      "[135]\ttrain-merror:0.06140\teval-merror:0.07764\n",
      "[136]\ttrain-merror:0.06135\teval-merror:0.07823\n",
      "[137]\ttrain-merror:0.06106\teval-merror:0.07808\n",
      "[138]\ttrain-merror:0.06106\teval-merror:0.07779\n",
      "[139]\ttrain-merror:0.06066\teval-merror:0.07793\n",
      "[140]\ttrain-merror:0.06062\teval-merror:0.07734\n",
      "[141]\ttrain-merror:0.06052\teval-merror:0.07749\n",
      "[142]\ttrain-merror:0.06052\teval-merror:0.07734\n",
      "[143]\ttrain-merror:0.06047\teval-merror:0.07734\n",
      "[144]\ttrain-merror:0.06037\teval-merror:0.07734\n",
      "[145]\ttrain-merror:0.06032\teval-merror:0.07749\n",
      "[146]\ttrain-merror:0.06007\teval-merror:0.07720\n",
      "[147]\ttrain-merror:0.06012\teval-merror:0.07734\n",
      "[148]\ttrain-merror:0.05993\teval-merror:0.07720\n",
      "[149]\ttrain-merror:0.05978\teval-merror:0.07734\n",
      "[150]\ttrain-merror:0.05983\teval-merror:0.07749\n",
      "[151]\ttrain-merror:0.05939\teval-merror:0.07749\n",
      "[152]\ttrain-merror:0.05904\teval-merror:0.07749\n",
      "[153]\ttrain-merror:0.05904\teval-merror:0.07764\n",
      "[154]\ttrain-merror:0.05894\teval-merror:0.07720\n",
      "[155]\ttrain-merror:0.05894\teval-merror:0.07720\n",
      "[156]\ttrain-merror:0.05865\teval-merror:0.07705\n",
      "[157]\ttrain-merror:0.05820\teval-merror:0.07705\n",
      "[158]\ttrain-merror:0.05820\teval-merror:0.07705\n",
      "[159]\ttrain-merror:0.05796\teval-merror:0.07720\n",
      "[160]\ttrain-merror:0.05786\teval-merror:0.07734\n",
      "[161]\ttrain-merror:0.05771\teval-merror:0.07734\n",
      "[162]\ttrain-merror:0.05766\teval-merror:0.07749\n",
      "[163]\ttrain-merror:0.05732\teval-merror:0.07764\n",
      "[164]\ttrain-merror:0.05722\teval-merror:0.07779\n",
      "[165]\ttrain-merror:0.05737\teval-merror:0.07779\n",
      "[166]\ttrain-merror:0.05722\teval-merror:0.07764\n",
      "[167]\ttrain-merror:0.05702\teval-merror:0.07808\n",
      "[168]\ttrain-merror:0.05702\teval-merror:0.07779\n",
      "[169]\ttrain-merror:0.05683\teval-merror:0.07793\n",
      "[170]\ttrain-merror:0.05693\teval-merror:0.07793\n",
      "[171]\ttrain-merror:0.05688\teval-merror:0.07793\n",
      "[172]\ttrain-merror:0.05658\teval-merror:0.07808\n",
      "[173]\ttrain-merror:0.05634\teval-merror:0.07779\n",
      "[174]\ttrain-merror:0.05624\teval-merror:0.07779\n",
      "[175]\ttrain-merror:0.05648\teval-merror:0.07764\n",
      "[176]\ttrain-merror:0.05624\teval-merror:0.07734\n",
      "Stopping. Best iteration:\n",
      "[156]\ttrain-merror:0.05865\teval-merror:0.07705\n",
      "\n",
      "[11:13:33] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-merror:0.09422\teval-merror:0.09845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'eval-merror' will be used for early stopping.\n",
      "\n",
      "Will train until eval-merror hasn't improved in 20 rounds.\n",
      "[1]\ttrain-merror:0.09284\teval-merror:0.09476\n",
      "[2]\ttrain-merror:0.09284\teval-merror:0.09432\n",
      "[3]\ttrain-merror:0.09201\teval-merror:0.09520\n",
      "[4]\ttrain-merror:0.09181\teval-merror:0.09506\n",
      "[5]\ttrain-merror:0.09146\teval-merror:0.09594\n",
      "[6]\ttrain-merror:0.09171\teval-merror:0.09609\n",
      "[7]\ttrain-merror:0.09137\teval-merror:0.09520\n",
      "[8]\ttrain-merror:0.09087\teval-merror:0.09535\n",
      "[9]\ttrain-merror:0.09107\teval-merror:0.09476\n",
      "[10]\ttrain-merror:0.09097\teval-merror:0.09520\n",
      "[11]\ttrain-merror:0.09063\teval-merror:0.09506\n",
      "[12]\ttrain-merror:0.09053\teval-merror:0.09506\n",
      "[13]\ttrain-merror:0.09053\teval-merror:0.09506\n",
      "[14]\ttrain-merror:0.09028\teval-merror:0.09461\n",
      "[15]\ttrain-merror:0.09023\teval-merror:0.09432\n",
      "[16]\ttrain-merror:0.09019\teval-merror:0.09461\n",
      "[17]\ttrain-merror:0.09023\teval-merror:0.09402\n",
      "[18]\ttrain-merror:0.08999\teval-merror:0.09388\n",
      "[19]\ttrain-merror:0.09009\teval-merror:0.09417\n",
      "[20]\ttrain-merror:0.09009\teval-merror:0.09432\n",
      "[21]\ttrain-merror:0.08979\teval-merror:0.09417\n",
      "[22]\ttrain-merror:0.08974\teval-merror:0.09432\n",
      "[23]\ttrain-merror:0.08974\teval-merror:0.09417\n",
      "[24]\ttrain-merror:0.08959\teval-merror:0.09432\n",
      "[25]\ttrain-merror:0.08905\teval-merror:0.09358\n",
      "[26]\ttrain-merror:0.08920\teval-merror:0.09328\n",
      "[27]\ttrain-merror:0.08768\teval-merror:0.09137\n",
      "[28]\ttrain-merror:0.08615\teval-merror:0.09077\n",
      "[29]\ttrain-merror:0.08581\teval-merror:0.09063\n",
      "[30]\ttrain-merror:0.08581\teval-merror:0.09092\n",
      "[31]\ttrain-merror:0.08576\teval-merror:0.09107\n",
      "[32]\ttrain-merror:0.08571\teval-merror:0.09092\n",
      "[33]\ttrain-merror:0.08561\teval-merror:0.09107\n",
      "[34]\ttrain-merror:0.08551\teval-merror:0.09107\n",
      "[35]\ttrain-merror:0.08492\teval-merror:0.09077\n",
      "[36]\ttrain-merror:0.08428\teval-merror:0.09092\n",
      "[37]\ttrain-merror:0.08330\teval-merror:0.09063\n",
      "[38]\ttrain-merror:0.08280\teval-merror:0.09019\n",
      "[39]\ttrain-merror:0.08177\teval-merror:0.08930\n",
      "[40]\ttrain-merror:0.08177\teval-merror:0.08930\n",
      "[41]\ttrain-merror:0.08162\teval-merror:0.08915\n",
      "[42]\ttrain-merror:0.08133\teval-merror:0.08856\n",
      "[43]\ttrain-merror:0.08074\teval-merror:0.08856\n",
      "[44]\ttrain-merror:0.08054\teval-merror:0.08871\n",
      "[45]\ttrain-merror:0.07941\teval-merror:0.08782\n",
      "[46]\ttrain-merror:0.07882\teval-merror:0.08723\n",
      "[47]\ttrain-merror:0.07862\teval-merror:0.08649\n",
      "[48]\ttrain-merror:0.07818\teval-merror:0.08561\n",
      "[49]\ttrain-merror:0.07744\teval-merror:0.08590\n",
      "[50]\ttrain-merror:0.07724\teval-merror:0.08546\n",
      "[51]\ttrain-merror:0.07690\teval-merror:0.08487\n",
      "[52]\ttrain-merror:0.07670\teval-merror:0.08487\n",
      "[53]\ttrain-merror:0.07656\teval-merror:0.08487\n",
      "[54]\ttrain-merror:0.07621\teval-merror:0.08428\n",
      "[55]\ttrain-merror:0.07592\teval-merror:0.08443\n",
      "[56]\ttrain-merror:0.07547\teval-merror:0.08428\n",
      "[57]\ttrain-merror:0.07537\teval-merror:0.08428\n",
      "[58]\ttrain-merror:0.07493\teval-merror:0.08428\n",
      "[59]\ttrain-merror:0.07454\teval-merror:0.08413\n",
      "[60]\ttrain-merror:0.07424\teval-merror:0.08413\n",
      "[61]\ttrain-merror:0.07380\teval-merror:0.08384\n",
      "[62]\ttrain-merror:0.07346\teval-merror:0.08413\n",
      "[63]\ttrain-merror:0.07351\teval-merror:0.08384\n",
      "[64]\ttrain-merror:0.07316\teval-merror:0.08339\n",
      "[65]\ttrain-merror:0.07306\teval-merror:0.08339\n",
      "[66]\ttrain-merror:0.07277\teval-merror:0.08384\n",
      "[67]\ttrain-merror:0.07262\teval-merror:0.08384\n",
      "[68]\ttrain-merror:0.07237\teval-merror:0.08413\n",
      "[69]\ttrain-merror:0.07233\teval-merror:0.08443\n",
      "[70]\ttrain-merror:0.07198\teval-merror:0.08384\n",
      "[71]\ttrain-merror:0.07193\teval-merror:0.08399\n",
      "[72]\ttrain-merror:0.07173\teval-merror:0.08399\n",
      "[73]\ttrain-merror:0.07110\teval-merror:0.08428\n",
      "[74]\ttrain-merror:0.07090\teval-merror:0.08339\n",
      "[75]\ttrain-merror:0.07075\teval-merror:0.08413\n",
      "[76]\ttrain-merror:0.07085\teval-merror:0.08413\n",
      "[77]\ttrain-merror:0.07046\teval-merror:0.08399\n",
      "[78]\ttrain-merror:0.07041\teval-merror:0.08413\n",
      "[79]\ttrain-merror:0.07011\teval-merror:0.08413\n",
      "[80]\ttrain-merror:0.07006\teval-merror:0.08413\n",
      "[81]\ttrain-merror:0.06996\teval-merror:0.08443\n",
      "[82]\ttrain-merror:0.07011\teval-merror:0.08369\n",
      "[83]\ttrain-merror:0.06986\teval-merror:0.08354\n",
      "[84]\ttrain-merror:0.06937\teval-merror:0.08325\n",
      "[85]\ttrain-merror:0.06913\teval-merror:0.08310\n",
      "[86]\ttrain-merror:0.06922\teval-merror:0.08325\n",
      "[87]\ttrain-merror:0.06913\teval-merror:0.08310\n",
      "[88]\ttrain-merror:0.06878\teval-merror:0.08339\n",
      "[89]\ttrain-merror:0.06844\teval-merror:0.08295\n",
      "[90]\ttrain-merror:0.06849\teval-merror:0.08280\n",
      "[91]\ttrain-merror:0.06790\teval-merror:0.08266\n",
      "[92]\ttrain-merror:0.06755\teval-merror:0.08280\n",
      "[93]\ttrain-merror:0.06735\teval-merror:0.08251\n",
      "[94]\ttrain-merror:0.06745\teval-merror:0.08236\n",
      "[95]\ttrain-merror:0.06711\teval-merror:0.08207\n",
      "[96]\ttrain-merror:0.06716\teval-merror:0.08162\n",
      "[97]\ttrain-merror:0.06706\teval-merror:0.08162\n",
      "[98]\ttrain-merror:0.06657\teval-merror:0.08192\n",
      "[99]\ttrain-merror:0.06642\teval-merror:0.08177\n",
      "[100]\ttrain-merror:0.06652\teval-merror:0.08192\n",
      "[101]\ttrain-merror:0.06642\teval-merror:0.08192\n",
      "[102]\ttrain-merror:0.06627\teval-merror:0.08207\n",
      "[103]\ttrain-merror:0.06613\teval-merror:0.08221\n",
      "[104]\ttrain-merror:0.06583\teval-merror:0.08177\n",
      "[105]\ttrain-merror:0.06583\teval-merror:0.08192\n",
      "[106]\ttrain-merror:0.06573\teval-merror:0.08207\n",
      "[107]\ttrain-merror:0.06539\teval-merror:0.08133\n",
      "[108]\ttrain-merror:0.06539\teval-merror:0.08177\n",
      "[109]\ttrain-merror:0.06489\teval-merror:0.08162\n",
      "[110]\ttrain-merror:0.06475\teval-merror:0.08148\n",
      "[111]\ttrain-merror:0.06480\teval-merror:0.08133\n",
      "[112]\ttrain-merror:0.06489\teval-merror:0.08103\n",
      "[113]\ttrain-merror:0.06480\teval-merror:0.08148\n",
      "[114]\ttrain-merror:0.06475\teval-merror:0.08148\n",
      "[115]\ttrain-merror:0.06455\teval-merror:0.08118\n",
      "[116]\ttrain-merror:0.06475\teval-merror:0.08162\n",
      "[117]\ttrain-merror:0.06426\teval-merror:0.08192\n",
      "[118]\ttrain-merror:0.06426\teval-merror:0.08148\n",
      "[119]\ttrain-merror:0.06421\teval-merror:0.08148\n",
      "[120]\ttrain-merror:0.06401\teval-merror:0.08118\n",
      "[121]\ttrain-merror:0.06396\teval-merror:0.08089\n",
      "[122]\ttrain-merror:0.06371\teval-merror:0.08103\n",
      "[123]\ttrain-merror:0.06371\teval-merror:0.08059\n",
      "[124]\ttrain-merror:0.06337\teval-merror:0.08103\n",
      "[125]\ttrain-merror:0.06337\teval-merror:0.08103\n",
      "[126]\ttrain-merror:0.06303\teval-merror:0.08103\n",
      "[127]\ttrain-merror:0.06278\teval-merror:0.08148\n",
      "[128]\ttrain-merror:0.06278\teval-merror:0.08133\n",
      "[129]\ttrain-merror:0.06268\teval-merror:0.08118\n",
      "[130]\ttrain-merror:0.06243\teval-merror:0.08103\n",
      "[131]\ttrain-merror:0.06219\teval-merror:0.08103\n",
      "[132]\ttrain-merror:0.06224\teval-merror:0.08118\n",
      "[133]\ttrain-merror:0.06160\teval-merror:0.08089\n",
      "[134]\ttrain-merror:0.06140\teval-merror:0.08074\n",
      "[135]\ttrain-merror:0.06106\teval-merror:0.08089\n",
      "[136]\ttrain-merror:0.06106\teval-merror:0.08074\n",
      "[137]\ttrain-merror:0.06086\teval-merror:0.08074\n",
      "[138]\ttrain-merror:0.06066\teval-merror:0.08089\n",
      "[139]\ttrain-merror:0.06027\teval-merror:0.08059\n",
      "[140]\ttrain-merror:0.06037\teval-merror:0.08044\n",
      "[141]\ttrain-merror:0.06012\teval-merror:0.08059\n",
      "[142]\ttrain-merror:0.06003\teval-merror:0.08074\n",
      "[143]\ttrain-merror:0.05998\teval-merror:0.08059\n",
      "[144]\ttrain-merror:0.05998\teval-merror:0.08015\n",
      "[145]\ttrain-merror:0.05973\teval-merror:0.08015\n",
      "[146]\ttrain-merror:0.05958\teval-merror:0.08015\n",
      "[147]\ttrain-merror:0.05953\teval-merror:0.08030\n",
      "[148]\ttrain-merror:0.05943\teval-merror:0.08015\n",
      "[149]\ttrain-merror:0.05909\teval-merror:0.08044\n",
      "[150]\ttrain-merror:0.05880\teval-merror:0.08044\n",
      "[151]\ttrain-merror:0.05874\teval-merror:0.08015\n",
      "[152]\ttrain-merror:0.05870\teval-merror:0.08074\n",
      "[153]\ttrain-merror:0.05840\teval-merror:0.08074\n",
      "[154]\ttrain-merror:0.05830\teval-merror:0.08015\n",
      "[155]\ttrain-merror:0.05815\teval-merror:0.08030\n",
      "[156]\ttrain-merror:0.05791\teval-merror:0.08015\n",
      "[157]\ttrain-merror:0.05751\teval-merror:0.08044\n",
      "[158]\ttrain-merror:0.05751\teval-merror:0.08044\n",
      "[159]\ttrain-merror:0.05722\teval-merror:0.08015\n",
      "[160]\ttrain-merror:0.05712\teval-merror:0.08000\n",
      "[161]\ttrain-merror:0.05722\teval-merror:0.08000\n",
      "[162]\ttrain-merror:0.05742\teval-merror:0.07985\n",
      "[163]\ttrain-merror:0.05737\teval-merror:0.07941\n",
      "[164]\ttrain-merror:0.05722\teval-merror:0.07985\n",
      "[165]\ttrain-merror:0.05693\teval-merror:0.07970\n",
      "[166]\ttrain-merror:0.05678\teval-merror:0.07970\n",
      "[167]\ttrain-merror:0.05663\teval-merror:0.08015\n",
      "[168]\ttrain-merror:0.05634\teval-merror:0.07985\n",
      "[169]\ttrain-merror:0.05624\teval-merror:0.08015\n",
      "[170]\ttrain-merror:0.05634\teval-merror:0.08000\n",
      "[171]\ttrain-merror:0.05614\teval-merror:0.08030\n",
      "[172]\ttrain-merror:0.05604\teval-merror:0.08030\n",
      "[173]\ttrain-merror:0.05599\teval-merror:0.08015\n",
      "[174]\ttrain-merror:0.05570\teval-merror:0.07985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[175]\ttrain-merror:0.05540\teval-merror:0.07956\n",
      "[176]\ttrain-merror:0.05545\teval-merror:0.07956\n",
      "[177]\ttrain-merror:0.05545\teval-merror:0.07941\n",
      "[178]\ttrain-merror:0.05515\teval-merror:0.07941\n",
      "[179]\ttrain-merror:0.05520\teval-merror:0.07970\n",
      "[180]\ttrain-merror:0.05501\teval-merror:0.07985\n",
      "[181]\ttrain-merror:0.05471\teval-merror:0.07985\n",
      "[182]\ttrain-merror:0.05422\teval-merror:0.07985\n",
      "[183]\ttrain-merror:0.05427\teval-merror:0.08000\n",
      "Stopping. Best iteration:\n",
      "[163]\ttrain-merror:0.05737\teval-merror:0.07941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_1NNa = Model1NNsoftmax()\n",
    "pred_train_1NNa, preds_test_1NNa = predict_cv(model_1NNa, train_x_nn_poutcome,train_y_nn_poutcome,test_x_nn_poutcome )\n",
    "\n",
    "model_1NNb = Model1NN2softmax()\n",
    "pred_train_1NNb, preds_test_1NNb = predict_cv(model_1NNb, train_x_nn_poutcome, train_y_nn_poutcome,test_x_nn_poutcome)\n",
    "\n",
    "model_1xgbc = Model1xgbsoftmax()\n",
    "pred_train_1xgbc, preds_test_1xgbc = predict_cv(model_1xgbc, train_x_poutcome_xg,train_y_poutcome_xg,test_x_poutcome_xg)\n",
    "\n",
    "model_1xgbd = Model1xgb2softmax()\n",
    "pred_train_1xgbd, preds_teat_1xgbd = predict_cv(model_1xgbd, train_x_poutcome_xg, train_y_poutcome_xg,test_x_poutcome_xg)\n",
    "\n",
    "model_1knne = Model2KNN_p()\n",
    "pred_train_1knne, preds_test_1knne = predict_cv_classfier(model_1knne, train_x_poutcome_xg,train_y_poutcome_xg, test_x_poutcome_xg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.3178\n",
      "logloss: 2.3796\n",
      "logloss: 0.2190\n",
      "logloss: 0.2182\n",
      "logloss: 3.0327\n"
     ]
    }
   ],
   "source": [
    "print(f'logloss: {log_loss(train_y_nn_poutcome, pred_train_1NNa, eps=1e-7):.4f}')\n",
    "print(f'logloss: {log_loss(train_y_nn_poutcome, pred_train_1NNb, eps=1e-7):.4f}')\n",
    "print(f'logloss: {log_loss(train_y_poutcome_xg, pred_train_1xgbc, eps=1e-7):.4f}')\n",
    "print(f'logloss: {log_loss(train_y_poutcome_xg, pred_train_1xgbd, eps=1e-7):.4f}')\n",
    "loss_c = pd.DataFrame(pred_train_1knne)\n",
    "loss_c = pd.get_dummies(loss_c[0])\n",
    "print(f'logloss: {log_loss(train_y_poutcome_xg, loss_c, eps=1e-7):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1ramdomf = Model1ramdom()\n",
    "pred_train_1ram1, preds_test_1ram1 = predict_cv(model_1ramdomf, train_x_poutcome_xg, train_y_poutcome_f, test_x_poutcome_xg)\n",
    "\n",
    "pred_train_1ram2, preds_test_1ram2 = predict_cv(model_1ramdomf, train_x_poutcome_xg, train_y_poutcome_o, test_x_poutcome_xg)\n",
    "\n",
    "pred_train_1ram3, preds_test_1ram3 = predict_cv(model_1ramdomf, train_x_poutcome_xg, train_y_poutcome_s, test_x_poutcome_xg)\n",
    "\n",
    "pred_train_1ram4, preds_test_1ram4 = predict_cv(model_1ramdomf, train_x_poutcome_xg, train_y_poutcome_u, test_x_poutcome_xg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.1789\n",
      "logloss: 0.1160\n",
      "logloss: 0.0722\n",
      "logloss: 0.1690\n"
     ]
    }
   ],
   "source": [
    "print(f'logloss: {log_loss(train_y_poutcome_f, pred_train_1ram1, eps=1e-7):.4f}')\n",
    "print(f'logloss: {log_loss(train_y_poutcome_o, pred_train_1ram2, eps=1e-7):.4f}')\n",
    "print(f'logloss: {log_loss(train_y_poutcome_s, pred_train_1ram3, eps=1e-7):.4f}')\n",
    "print(f'logloss: {log_loss(train_y_poutcome_u, pred_train_1ram4, eps=1e-7):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = np.split(pred_train_1NNa,4, axis=1)\n",
    "df1 = pd.DataFrame(a[0])\n",
    "df1.columns = ['1NNa_failure']\n",
    "df1['1NNa_other'] = a[1]\n",
    "df1['1NNa_success'] = a[2]\n",
    "df1['1NNa_unknown'] = a[3]\n",
    "\n",
    "a2 = np.split(preds_test_1NNa,4, axis=1)\n",
    "df7 = pd.DataFrame(a2[0])\n",
    "df7.columns = ['1NNa_failure_test']\n",
    "df7['1NNa_other_test'] = a2[1]\n",
    "df7['1NNa_success_test'] = a2[2]\n",
    "df7['1NNa_unknown_test'] = a2[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.split(pred_train_1NNb,4, axis=1)\n",
    "df2 = pd.DataFrame(b[0])\n",
    "df2.columns = ['1NNb_failure']\n",
    "df2['1NNb_other'] = b[1]\n",
    "df2['1NNb_success'] = b[2]\n",
    "df2['1NNb_unknown'] = b[3]\n",
    "\n",
    "\n",
    "\n",
    "b2 = np.split(preds_test_1NNb,4, axis=1)\n",
    "df8 = pd.DataFrame(b2[0])\n",
    "df8.columns = ['1NNb_failure_test']\n",
    "df8['1NNb_other_test'] = b2[1]\n",
    "df8['1NNb_success_test'] = b2[2]\n",
    "df8['1NNb_unknown_test'] = b2[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.split(pred_train_1xgbc,4, axis=1)\n",
    "df3 = pd.DataFrame(c[0])\n",
    "df3.columns = ['1xgbc_failure']\n",
    "df3['1xgbc_other'] = c[1]\n",
    "df3['1xgbc_success'] = c[2]\n",
    "df3['1xgbc_unknown'] = c[3]\n",
    "\n",
    "\n",
    "\n",
    "c2 = np.split(preds_test_1xgbc,4, axis=1)\n",
    "df9 = pd.DataFrame(c2[0])\n",
    "df9.columns = ['1xgbc_failure_test']\n",
    "df9['1xgbc_other_test'] = c2[1]\n",
    "df9['1xgbc_success_test'] = c2[2]\n",
    "df9['1xgbc_unknown_test'] = c2[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.split(pred_train_1xgbd,4, axis=1)\n",
    "df4 = pd.DataFrame(d[0])\n",
    "df4.columns = ['1xgbd_failure']\n",
    "df4['1xgbd_other'] = d[1]\n",
    "df4['1xgbd_success'] = d[2]\n",
    "df4['1xgbd_unknown'] = d[3]\n",
    "\n",
    "\n",
    "\n",
    "d2 = np.split(preds_teat_1xgbd,4, axis=1)\n",
    "df10 = pd.DataFrame(d2[0])\n",
    "df10.columns = ['1xgbd_failure_test']\n",
    "df10['1xgbd_other_test'] = d2[1]\n",
    "df10['1xgbd_success_test'] = d2[2]\n",
    "df10['1xgbd_unknown_test'] = d2[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_c.columns = ['1knne_failure','1knne_other','1knne_success',\n",
    "                 '1knne_unknown']\n",
    "df5 = loss_c\n",
    "\n",
    "df11 = pd.DataFrame(preds_test_1knne)\n",
    "df11 = pd.get_dummies(df11[0])\n",
    "df11.columns = ['1knne_failure_test','1knne_other_test','1knne_success_test',\n",
    "                 '1knne_unknown_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = pd.DataFrame(pred_train_1ram1)\n",
    "df6.columns=['1ramdom_failure']\n",
    "df6['1ramdom_other'] = pred_train_1ram2\n",
    "df6['1ramdom_success'] = pred_train_1ram3\n",
    "df6['1ramdom_unknown'] = pred_train_1ram4\n",
    "\n",
    "\n",
    "df12 = pd.DataFrame(preds_test_1ram1)\n",
    "df12.columns=['1ramdom_failure_test']\n",
    "df12['1ramdom_other_test'] = preds_test_1ram2\n",
    "df12['1ramdom_success_test'] = preds_test_1ram3\n",
    "df12['1ramdom_unknown_test'] = preds_test_1ram4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x2 = pd.concat([df1,df2,df3,df4,df5,df6], axis = 1)\n",
    "test_x2 = pd.concat([df7,df8,df9,df10,df11,df12], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_poutcome_f = train_y_poutcome_dummies[0]\n",
    "train_y_poutcome_o = train_y_poutcome_dummies[1]\n",
    "train_y_poutcome_s = train_y_poutcome_dummies[2]\n",
    "train_y_poutcome_u = train_y_poutcome_dummies[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Model3logistic()\n",
    "pred_train_2_f, preds_test_2_f = predict_cv(model2, train_x2, train_y_poutcome_f, test_x2)\n",
    "pred_train_2_o, preds_test_2_o = predict_cv(model2, train_x2, train_y_poutcome_o, test_x2)\n",
    "pred_train_2_s, preds_test_2_s = predict_cv(model2, train_x2, train_y_poutcome_s, test_x2)\n",
    "pred_train_2_u, preds_test_2_u = predict_cv(model2, train_x2, train_y_poutcome_u, test_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.1392\n",
      "logloss: 0.0886\n",
      "logloss: 0.0489\n",
      "logloss: 0.1320\n"
     ]
    }
   ],
   "source": [
    "print(f'logloss: {log_loss(train_y_poutcome_f, pred_train_2_f, eps=1e-7):.4f}')\n",
    "print(f'logloss: {log_loss(train_y_poutcome_o, pred_train_2_o, eps=1e-7):.4f}')\n",
    "print(f'logloss: {log_loss(train_y_poutcome_s, pred_train_2_s, eps=1e-7):.4f}')\n",
    "print(f'logloss: {log_loss(train_y_poutcome_u, pred_train_2_u, eps=1e-7):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df13 = pd.DataFrame(pred_train_2_f)\n",
    "df13.columns=['stacking_f']\n",
    "df13['stacking_o'] = pred_train_2_o\n",
    "df13['stacking_s'] = pred_train_2_s\n",
    "df13['stacking_u'] = pred_train_2_u\n",
    "\n",
    "\n",
    "\n",
    "df14 = pd.DataFrame(preds_test_2_f)\n",
    "df14.columns=['stacking_f_test']\n",
    "df14['stacking_o_test'] = preds_test_2_o\n",
    "df14['stacking_s_test'] = preds_test_2_s\n",
    "df14['stacking_u_test'] = preds_test_2_u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_new = train_x.drop(['poutcome'], axis = 1)\n",
    "train_x_new = pd.concat([train_x_new, df13], axis=1)\n",
    "\n",
    "train_x_nn_new = train_x_nn.iloc[:,:48]\n",
    "train_x_nn_new = pd.concat([train_x_nn_new, df13], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_new = test_x.drop(['poutcome'], axis = 1)\n",
    "test_x_new = pd.concat([test_x_new, df14], axis=1)\n",
    "\n",
    "test_x_nn_new = test_x_nn.iloc[:,:48]\n",
    "test_x_nn_new = pd.concat([test_x_nn_new, df14], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_new.to_csv('train_x_p_stack.csv')\n",
    "test_x_new.to_csv('test_x_p_stack.csv')\n",
    "train_x_nn.to_csv('train_x_nn_stack.csv')\n",
    "test_x_nn_new.to_csv('test_x_nn_stack.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit",
   "language": "python",
   "name": "python37164bit745322e800af484d9e7663a15d4e0767"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
